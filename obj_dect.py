{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset_factory import get_dataset\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "get_dataset=get_dataset('pascal','ctdet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opts():\n",
    "    data_dir='/home/lishiqi/obj/CenterNet/src/lib/../../data'\n",
    "    keep_res=False\n",
    "    input_h=512\n",
    "    input_w=512\n",
    "    down_ratio=4\n",
    "    mse_loss=False\n",
    "    dense_wh=False\n",
    "    cat_spec_wh=False\n",
    "    reg_offset=True\n",
    "    debug=0\n",
    "    draw_ma_gaussian=True\n",
    "    not_rand_crop=True\n",
    "    scale=0\n",
    "    shift=0\n",
    "    flip=0\n",
    "    no_color_aug=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> initializing pascal trainval0712 data.\n",
      "loading annotations into memory...\n",
      "Done (t=0.97s)\n",
      "creating index...\n",
      "index created!\n",
      "Loaded train 16551 samples\n"
     ]
    }
   ],
   "source": [
    "opt=Opts()\n",
    "dataset_pascal=get_dataset(opt,'train')\n",
    "objs=torch.load('objs_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PascalDataset(Dataset):\n",
    "    def __init__(self,dataset,obj_res,pic_res,objs):\n",
    "        self.objs=objs['objs_'+str(obj_res)]['pics_'+str(pic_res)]\n",
    "        self.dataset=dataset\n",
    "        self.obj_res=obj_res\n",
    "        self.pic_res=pic_res\n",
    "        self.avgpool=torch.nn.AvgPool2d(4,4)\n",
    "        self.maxpool=torch.nn.MaxPool2d(4,4)\n",
    "        self.max_objs=50\n",
    "    def __len__(self):\n",
    "        return len(self.objs)\n",
    "    def __getitem__(self, index):\n",
    "        dataset=self.dataset\n",
    "        objs=self.objs\n",
    "        pic_index=objs[index]['pic_index']\n",
    "        obj_index=objs[index]['obj_index']\n",
    "        obj_pic=dataset[pic_index]\n",
    "        \n",
    "        wh=obj_pic['wh'][obj_index]*4\n",
    "        ori_pic=obj_pic['input']\n",
    "        bbox=(obj_pic['bboxs'][obj_index]*4).astype(np.int)\n",
    "        hms=obj_pic['hm']\n",
    "        bbox_crop=self.crop_pic(ori_pic,bbox,wh)\n",
    "        batch_dict=self.gen_offset(obj_pic['bboxs'],bbox_crop)\n",
    "        crop_pic=ori_pic[:,bbox_crop[1]:bbox_crop[3],bbox_crop[0]:bbox_crop[2]]\n",
    "        crop_hm=hms[:,bbox_crop[1]:bbox_crop[3],bbox_crop[0]:bbox_crop[2]]\n",
    "        crop_resize_hm=self.avgpool(torch.Tensor(crop_hm))\n",
    "        batch_dict['hm']=crop_resize_hm\n",
    "        batch_dict['input']=crop_pic\n",
    "        return batch_dict\n",
    "    def gen_offset(self,obj_bboxes,bbox):\n",
    "        reg = np.zeros((self.max_objs, 2), dtype=np.float32)\n",
    "        ind = np.zeros((self.max_objs), dtype=np.int64)\n",
    "        reg_mask = np.zeros((self.max_objs), dtype=np.uint8)\n",
    "        wh=np.zeros((self.max_objs,2),dtype=np.float32)\n",
    "        \n",
    "        bbox_output=bbox/4\n",
    "        w=bbox_output[2]-bbox_output[0]\n",
    "        h=bbox_output[3]-bbox_output[1]\n",
    "\n",
    "        for index,obj_bbox in enumerate(obj_bboxes):\n",
    "            if obj_bbox[0]>=bbox_output[0] and obj_bbox[1]>=bbox_output[1] and obj_bbox[2]<=bbox_output[2] and obj_bbox[3]<=bbox_output[3]:\n",
    "                obj_bbox_offset=obj_bbox-[bbox_output[0],bbox_output[1],bbox_output[0],bbox_output[1]]\n",
    "\n",
    "                ct = np.array([(obj_bbox_offset[0] + obj_bbox_offset[2]) / 2, (obj_bbox_offset[1] + obj_bbox_offset[3]) / 2], dtype=np.float32)\n",
    "                ct_int = ct.astype(np.int32)\n",
    "                reg[index]=ct-ct_int\n",
    "                index_ct=ct_int[1]*w+ct_int[0]\n",
    "                wh[index]=[obj_bbox_offset[2]-obj_bbox_offset[0],obj_bbox_offset[3]-obj_bbox_offset[1]]\n",
    "                if index<0 or index>=w*h:\n",
    "                    reg_mask[index]=0\n",
    "                    ind[index]=0\n",
    "                else:\n",
    "                    reg_mask[index]=1\n",
    "                    ind[index]=index_ct\n",
    "        wh_and_offset=dict(reg=reg,ind=ind,reg_mask=reg_mask,wh=wh)\n",
    "        return wh_and_offset\n",
    "                \n",
    "    def crop_pic(self,pic,bbox,wh):\n",
    "        bbox_crop=np.zeros(4,np.int)\n",
    "        ori_w=pic.shape[2]\n",
    "        ori_h=pic.shape[1]\n",
    "        cut_w,cut_h=self.pic_res,self.pic_res\n",
    "        if bbox[1]<ori_h-bbox[3]:\n",
    "            max_h,h_l=bbox[1],True\n",
    "            min_h=(cut_h+bbox[1])-ori_h\n",
    "        else:\n",
    "            max_h,h_l=(ori_h-bbox[3]),False\n",
    "            min_h=cut_h-bbox[3]\n",
    "        if bbox[0]<ori_w-bbox[2]:\n",
    "            max_w,w_l=bbox[0],True\n",
    "            min_w=(cut_w+bbox[0])-ori_w\n",
    "        else:\n",
    "            max_w,w_l=(ori_w-bbox[2]),False\n",
    "            min_w=cut_w-bbox[2]\n",
    "        max_h=min(max_h,cut_h-wh[1])\n",
    "        max_w=min(max_w,cut_w-wh[0])\n",
    "        min_h=max(0,min_h)\n",
    "        min_w=max(0,min_w)\n",
    "        \n",
    "        rand_h=np.random.randint(min_h,max_h+1)\n",
    "        rand_w=np.random.randint(min_w,max_w+1)\n",
    "\n",
    "        if h_l:\n",
    "            bbox_crop[1]=bbox[1]-rand_h\n",
    "            bbox_crop[3]=bbox_crop[1]+cut_h\n",
    "        else:\n",
    "            bbox_crop[3]=bbox[3]+rand_h\n",
    "            bbox_crop[1]=bbox_crop[3]-cut_h\n",
    "        if w_l:\n",
    "            bbox_crop[0]=bbox[0]-rand_w\n",
    "            bbox_crop[2]=bbox_crop[0]+cut_w\n",
    "        else:\n",
    "            bbox_crop[2]=bbox[2]+rand_w\n",
    "            bbox_crop[0]=bbox_crop[2]-cut_w\n",
    "\n",
    "        return bbox_crop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DatasetObj(Dataset):\n",
    "    def __init__(self,dataset_obj,dataset,objs,obj_res,pic_res,loader_bses):\n",
    "        dataloaders=[]\n",
    "        dataloader_size=[]\n",
    "        for res,loader_bs in zip(pic_res,loader_bses):\n",
    "            dataloaders.append(iter(DataLoader(dataset_obj(dataset,obj_res,res,objs),batch_size=loader_bs,shuffle=True)))\n",
    "        sum_len=0\n",
    "        for dataloader in dataloaders: \n",
    "            sum_len+=len(dataloader)\n",
    "            dataloader_size.append(len(dataloader))\n",
    "        self.dataloaders=dataloaders\n",
    "        self.dataloader_size=dataloader_size\n",
    "        self.sum_len=sum_len\n",
    "        self.random_pool=self.dataloader_size.copy()\n",
    "    def __getitem__(self,index):\n",
    "        indexs =[i for i in range(len(self.random_pool)) if self.random_pool[i]>0]\n",
    "        loader_index=random.choice(indexs)\n",
    "    \n",
    "        self.random_pool[loader_index]-=1\n",
    "        batch=self.dataloaders[loader_index].next()\n",
    "        if index==self.sum_len-1:\n",
    "            self.random_pool=self.dataloader_size.copy()\n",
    "        return batch\n",
    "    def __len__(self):\n",
    "        return self.sum_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetObjMuiltRes(Dataset):\n",
    "    def __init__(self,objs,dataset,obj_res=[],pic_res=dict(),loader_bses=dict()):\n",
    "        self.obj_res=obj_res\n",
    "        dataloaders=[]\n",
    "        def default_collate(batch):\n",
    "            return batch[0]\n",
    "        for res in obj_res:\n",
    "            dataset_obj=DatasetObj(PascalDataset,dataset,objs,res,pic_res['pic_'+str(res)],loader_bses['pic_'+str(res)])\n",
    "            dataloaders.append(iter(DataLoader(dataset_obj,num_workers=0,collate_fn=default_collate)))\n",
    "        dataloader_size=[]\n",
    "        sum_len=0\n",
    "        for dataloader in dataloaders: \n",
    "            sum_len+=len(dataloader)\n",
    "            dataloader_size.append(len(dataloader))\n",
    "        self.dataloaders=dataloaders\n",
    "        self.dataloader_size=dataloader_size\n",
    "        self.sum_len=sum_len\n",
    "        self.random_pool=self.dataloader_size.copy()\n",
    "    def __getitem__(self,index):\n",
    "        indexs =[i for i in range(len(self.random_pool)) if self.random_pool[i]>0]\n",
    "        loader_index=random.choice(indexs)\n",
    "        self.random_pool[loader_index]-=1\n",
    "        batch=self.dataloaders[loader_index].next()\n",
    "        if index==self.sum_len-1:\n",
    "            self.random_pool=self.dataloader_size.copy()\n",
    "        batch['res']=self.obj_res[loader_index]\n",
    "        return batch\n",
    "    def __len__(self):\n",
    "        return self.sum_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=DatasetObjMuiltRes(objs,\n",
    "                        dataset_pascal,\n",
    "                   obj_res=[32,64,128,256],\n",
    "                   pic_res=dict(pic_32=[64,128,192,256],\n",
    "                                pic_64=[128,192,256,384],\n",
    "                                pic_128=[256,384,512],\n",
    "                                pic_256=[384,512]),\n",
    "                   loader_bses=dict(pic_32=[128,48,16,8],\n",
    "                                    pic_64=[128,64,32,16],\n",
    "                                    pic_128=[64,32,8],\n",
    "                                    pic_256=[32,16]))\n",
    "def default_collate(batch):\n",
    "    return batch[0]\n",
    "loader=DataLoader(data,num_workers=10,collate_fn=default_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8 32 256\n",
      "1 8 128 512\n",
      "2 64 64 192\n",
      "3 32 64 256\n",
      "4 128 32 64\n",
      "5 128 32 64\n",
      "6 64 64 192\n",
      "7 32 256 384\n",
      "8 32 256 384\n",
      "9 64 128 256\n",
      "10 64 128 256\n",
      "11 32 128 384\n",
      "12 16 64 384\n",
      "13 16 256 512\n",
      "14 32 128 384\n",
      "15 16 256 512\n",
      "16 32 256 384\n",
      "17 8 32 256\n",
      "18 128 64 128\n",
      "19 64 64 192\n",
      "20 128 64 128\n",
      "21 32 64 256\n",
      "22 16 64 384\n",
      "23 16 64 384\n",
      "24 16 256 512\n",
      "25 48 32 128\n",
      "26 32 256 384\n",
      "27 16 256 512\n",
      "28 32 256 384\n",
      "29 128 32 64\n",
      "30 128 64 128\n",
      "31 8 128 512\n",
      "32 16 256 512\n",
      "33 16 256 512\n",
      "34 16 256 512\n",
      "35 8 32 256\n",
      "36 16 256 512\n",
      "37 32 256 384\n",
      "38 16 256 512\n",
      "39 32 256 384\n",
      "40 16 64 384\n",
      "41 8 32 256\n",
      "42 128 32 64\n",
      "43 128 32 64\n",
      "44 48 32 128\n",
      "45 16 64 384\n",
      "46 16 32 192\n",
      "47 16 32 192\n",
      "48 32 64 256\n",
      "49 16 64 384\n",
      "50 8 128 512\n",
      "51 128 32 64\n",
      "52 16 32 192\n",
      "53 16 256 512\n",
      "54 8 128 512\n",
      "55 16 256 512\n",
      "56 16 256 512\n",
      "57 16 64 384\n",
      "58 128 64 128\n",
      "59 64 128 256\n",
      "60 128 64 128\n",
      "61 16 256 512\n",
      "62 64 64 192\n",
      "63 8 128 512\n",
      "64 8 32 256\n",
      "65 64 128 256\n",
      "66 32 128 384\n",
      "67 128 32 64\n",
      "68 32 64 256\n",
      "69 64 128 256\n",
      "70 64 128 256\n",
      "71 16 32 192\n",
      "72 64 128 256\n",
      "73 32 256 384\n",
      "74 16 256 512\n",
      "75 128 64 128\n",
      "76 16 32 192\n",
      "77 32 256 384\n",
      "78 16 32 192\n",
      "79 128 64 128\n",
      "80 8 128 512\n",
      "81 8 32 256\n",
      "82 64 128 256\n",
      "83 32 256 384\n",
      "84 64 64 192\n",
      "85 8 32 256\n",
      "86 16 32 192\n",
      "87 128 32 64\n",
      "88 128 32 64\n",
      "89 16 256 512\n",
      "90 128 32 64\n",
      "91 16 256 512\n",
      "92 32 256 384\n",
      "93 8 128 512\n",
      "94 8 32 256\n",
      "95 32 64 256\n",
      "96 8 128 512\n",
      "97 32 256 384\n",
      "98 128 64 128\n",
      "99 64 128 256\n",
      "100 128 64 128\n",
      "101 16 256 512\n",
      "102 32 64 256\n",
      "103 16 256 512\n",
      "104 32 256 384\n",
      "105 64 128 256\n",
      "106 64 64 192\n",
      "107 32 256 384\n",
      "108 64 128 256\n",
      "109 32 128 384\n",
      "110 128 64 128\n",
      "111 32 128 384\n",
      "112 16 64 384\n",
      "113 64 128 256\n",
      "114 128 64 128\n",
      "115 64 128 256\n",
      "116 64 128 256\n",
      "117 128 32 64\n",
      "118 8 128 512\n",
      "119 8 128 512\n",
      "120 16 64 384\n",
      "121 32 128 384\n",
      "122 128 32 64\n",
      "123 32 64 256\n",
      "124 32 256 384\n",
      "125 32 256 384\n",
      "126 32 256 384\n",
      "127 128 32 64\n",
      "128 32 128 384\n",
      "129 48 32 128\n",
      "130 128 32 64\n",
      "131 8 128 512\n",
      "132 8 128 512\n",
      "133 16 256 512\n",
      "134 48 32 128\n",
      "135 16 32 192\n",
      "136 32 256 384\n",
      "137 128 64 128\n",
      "138 32 64 256\n",
      "139 32 256 384\n",
      "140 64 64 192\n",
      "141 16 64 384\n",
      "142 8 128 512\n",
      "143 16 256 512\n",
      "144 8 32 256\n",
      "145 8 128 512\n",
      "146 8 128 512\n",
      "147 32 256 384\n",
      "148 16 64 384\n",
      "149 16 256 512\n",
      "150 48 32 128\n",
      "151 32 64 256\n",
      "152 128 64 128\n",
      "153 16 64 384\n",
      "154 32 64 256\n",
      "155 128 32 64\n",
      "156 16 256 512\n",
      "157 16 256 512\n",
      "158 48 32 128\n",
      "159 64 64 192\n",
      "160 8 128 512\n",
      "161 64 64 192\n",
      "162 64 128 256\n",
      "163 16 256 512\n",
      "164 8 128 512\n",
      "165 16 256 512\n",
      "166 16 256 512\n",
      "167 16 32 192\n",
      "168 16 64 384\n",
      "169 32 256 384\n",
      "170 8 128 512\n",
      "171 16 32 192\n",
      "172 32 256 384\n",
      "173 16 256 512\n",
      "174 32 64 256\n",
      "175 128 64 128\n",
      "176 128 64 128\n",
      "177 32 128 384\n",
      "178 32 128 384\n",
      "179 128 32 64\n",
      "180 16 32 192\n",
      "181 128 32 64\n",
      "182 8 128 512\n",
      "183 128 32 64\n",
      "184 32 128 384\n",
      "185 16 64 384\n",
      "186 32 256 384\n",
      "187 16 256 512\n",
      "188 64 128 256\n",
      "189 128 32 64\n",
      "190 32 256 384\n",
      "191 64 64 192\n",
      "192 64 64 192\n",
      "193 32 128 384\n",
      "194 8 128 512\n",
      "195 32 256 384\n",
      "196 64 64 192\n",
      "197 16 256 512\n",
      "198 128 64 128\n",
      "199 32 64 256\n",
      "200 64 64 192\n",
      "201 8 128 512\n",
      "202 16 64 384\n",
      "203 16 64 384\n",
      "204 16 64 384\n",
      "205 16 256 512\n",
      "206 16 256 512\n",
      "207 32 128 384\n",
      "208 32 128 384\n",
      "209 16 256 512\n",
      "210 16 256 512\n",
      "211 64 64 192\n",
      "212 128 32 64\n",
      "213 64 64 192\n",
      "214 64 128 256\n",
      "215 32 64 256\n",
      "216 32 256 384\n",
      "217 32 128 384\n",
      "218 8 128 512\n",
      "219 32 256 384\n",
      "220 32 128 384\n",
      "221 32 64 256\n",
      "222 64 64 192\n",
      "223 32 256 384\n",
      "224 8 128 512\n",
      "225 16 32 192\n",
      "226 32 64 256\n",
      "227 32 256 384\n",
      "228 128 64 128\n",
      "229 16 64 384\n",
      "230 64 128 256\n",
      "231 128 32 64\n",
      "232 32 64 256\n",
      "233 64 128 256\n",
      "234 48 32 128\n",
      "235 16 32 192\n",
      "236 16 32 192\n",
      "237 128 64 128\n",
      "238 128 32 64\n",
      "239 48 32 128\n",
      "240 8 128 512\n",
      "241 64 128 256\n",
      "242 16 64 384\n",
      "243 32 256 384\n",
      "244 16 256 512\n",
      "245 128 32 64\n",
      "246 32 256 384\n",
      "247 32 256 384\n",
      "248 128 32 64\n",
      "249 8 128 512\n",
      "250 64 128 256\n",
      "251 16 64 384\n",
      "252 128 32 64\n",
      "253 64 128 256\n",
      "254 32 256 384\n",
      "255 16 256 512\n",
      "256 8 128 512\n",
      "257 128 64 128\n",
      "258 48 32 128\n",
      "259 64 128 256\n",
      "260 128 32 64\n",
      "261 8 128 512\n",
      "262 128 64 128\n",
      "263 64 64 192\n",
      "264 8 128 512\n",
      "265 8 128 512\n",
      "266 16 64 384\n",
      "267 16 256 512\n",
      "268 16 32 192\n",
      "269 16 256 512\n",
      "270 16 64 384\n",
      "271 8 128 512\n",
      "272 16 256 512\n",
      "273 128 64 128\n",
      "274 48 32 128\n",
      "275 16 64 384\n",
      "276 16 32 192\n",
      "277 128 64 128\n",
      "278 16 32 192\n",
      "279 16 64 384\n",
      "280 16 64 384\n",
      "281 128 64 128\n",
      "282 16 32 192\n",
      "283 64 64 192\n",
      "284 32 128 384\n",
      "285 32 256 384\n",
      "286 32 256 384\n",
      "287 64 128 256\n",
      "288 16 256 512\n",
      "289 48 32 128\n",
      "290 16 256 512\n",
      "291 64 128 256\n",
      "292 128 32 64\n",
      "293 64 64 192\n",
      "294 64 64 192\n",
      "295 16 256 512\n",
      "296 8 128 512\n",
      "297 8 32 256\n",
      "298 16 32 192\n",
      "299 32 256 384\n",
      "300 32 128 384\n",
      "301 32 256 384\n",
      "302 8 128 512\n",
      "303 64 128 256\n",
      "304 16 32 192\n",
      "305 16 64 384\n",
      "306 8 128 512\n",
      "307 48 32 128\n",
      "308 16 32 192\n",
      "309 16 32 192\n",
      "310 32 256 384\n",
      "311 64 128 256\n",
      "312 128 32 64\n",
      "313 48 32 128\n",
      "314 32 256 384\n",
      "315 32 128 384\n",
      "316 16 256 512\n",
      "317 128 64 128\n",
      "318 64 128 256\n",
      "319 48 32 128\n",
      "320 16 32 192\n",
      "321 16 32 192\n",
      "322 16 32 192\n",
      "323 16 256 512\n",
      "324 128 32 64\n",
      "325 128 64 128\n",
      "326 16 32 192\n",
      "327 16 256 512\n",
      "328 32 128 384\n",
      "329 8 128 512\n",
      "330 16 32 192\n",
      "331 32 128 384\n",
      "332 16 256 512\n",
      "333 64 64 192\n",
      "334 8 32 256\n",
      "335 8 128 512\n",
      "336 16 256 512\n",
      "337 16 256 512\n",
      "338 16 256 512\n",
      "339 16 64 384\n",
      "340 32 256 384\n",
      "341 16 64 384\n",
      "342 128 64 128\n",
      "343 16 32 192\n",
      "344 16 256 512\n",
      "345 16 64 384\n",
      "346 32 64 256\n",
      "347 32 64 256\n",
      "348 32 64 256\n",
      "349 48 32 128\n",
      "350 16 256 512\n",
      "351 16 256 512\n",
      "352 64 128 256\n",
      "353 16 32 192\n",
      "354 32 128 384\n",
      "355 128 32 64\n",
      "356 32 256 384\n",
      "357 32 64 256\n",
      "358 16 32 192\n",
      "359 32 256 384\n",
      "360 16 256 512\n",
      "361 8 32 256\n",
      "362 16 256 512\n",
      "363 8 32 256\n",
      "364 16 32 192\n",
      "365 128 64 128\n",
      "366 8 128 512\n",
      "367 16 32 192\n",
      "368 128 32 64\n",
      "369 16 32 192\n",
      "370 48 32 128\n",
      "371 16 32 192\n",
      "372 32 64 256\n",
      "373 64 128 256\n",
      "374 64 64 192\n",
      "375 8 32 256\n",
      "376 8 32 256\n",
      "377 8 32 256\n",
      "378 16 256 512\n",
      "379 48 32 128\n",
      "380 16 64 384\n",
      "381 32 256 384\n",
      "382 128 32 64\n",
      "383 32 256 384\n",
      "384 128 32 64\n",
      "385 48 32 128\n",
      "386 64 128 256\n",
      "387 16 64 384\n",
      "388 64 64 192\n",
      "389 64 64 192\n",
      "390 16 32 192\n",
      "391 16 256 512\n",
      "392 48 32 128\n",
      "393 16 32 192\n",
      "394 16 256 512\n",
      "395 32 64 256\n",
      "396 64 128 256\n",
      "397 48 32 128\n",
      "398 32 256 384\n",
      "399 16 256 512\n",
      "400 16 256 512\n",
      "401 16 32 192\n",
      "402 16 64 384\n",
      "403 16 32 192\n",
      "404 8 32 256\n",
      "405 128 64 128\n",
      "406 128 32 64\n",
      "407 32 128 384\n",
      "408 8 32 256\n",
      "409 128 32 64\n",
      "410 16 256 512\n",
      "411 16 256 512\n",
      "412 32 128 384\n",
      "413 64 128 256\n",
      "414 16 64 384\n",
      "415 32 256 384\n",
      "416 128 32 64\n",
      "417 16 32 192\n",
      "418 64 64 192\n",
      "419 64 128 256\n",
      "420 64 64 192\n",
      "421 32 128 384\n",
      "422 128 64 128\n",
      "423 16 64 384\n",
      "424 128 32 64\n",
      "425 64 128 256\n",
      "426 8 128 512\n",
      "427 128 32 64\n",
      "428 128 32 64\n",
      "429 16 256 512\n",
      "430 16 32 192\n",
      "431 32 256 384\n",
      "432 8 32 256\n",
      "433 16 256 512\n",
      "434 16 32 192\n",
      "435 16 32 192\n",
      "436 64 128 256\n",
      "437 32 128 384\n",
      "438 32 64 256\n",
      "439 32 64 256\n",
      "440 16 256 512\n",
      "441 48 32 128\n",
      "442 16 256 512\n",
      "443 8 128 512\n",
      "444 64 128 256\n",
      "445 64 128 256\n",
      "446 128 64 128\n",
      "447 16 256 512\n",
      "448 16 256 512\n",
      "449 16 256 512\n",
      "450 8 128 512\n",
      "451 8 128 512\n",
      "452 16 64 384\n",
      "453 32 64 256\n",
      "454 8 32 256\n",
      "455 32 256 384\n",
      "456 64 64 192\n",
      "457 16 64 384\n",
      "458 64 128 256\n",
      "459 128 64 128\n",
      "460 16 64 384\n",
      "461 32 256 384\n",
      "462 48 32 128\n",
      "463 48 32 128\n",
      "464 64 64 192\n",
      "465 8 32 256\n",
      "466 128 32 64\n",
      "467 64 128 256\n",
      "468 128 64 128\n",
      "469 128 32 64\n",
      "470 48 32 128\n",
      "471 64 64 192\n",
      "472 128 64 128\n",
      "473 8 128 512\n",
      "474 16 256 512\n",
      "475 16 64 384\n",
      "476 128 32 64\n",
      "477 128 64 128\n",
      "478 32 256 384\n",
      "479 16 256 512\n",
      "480 16 256 512\n",
      "481 64 128 256\n",
      "482 32 256 384\n",
      "483 32 128 384\n",
      "484 8 128 512\n",
      "485 8 128 512\n",
      "486 16 32 192\n",
      "487 64 128 256\n",
      "488 64 64 192\n",
      "489 16 32 192\n",
      "490 16 32 192\n",
      "491 8 128 512\n",
      "492 32 64 256\n",
      "493 32 256 384\n",
      "494 16 64 384\n",
      "495 32 64 256\n",
      "496 32 256 384\n",
      "497 16 256 512\n",
      "498 8 128 512\n",
      "499 64 64 192\n",
      "500 16 256 512\n",
      "501 128 64 128\n",
      "502 8 32 256\n",
      "503 32 256 384\n",
      "504 32 64 256\n",
      "505 32 64 256\n",
      "506 8 128 512\n",
      "507 8 32 256\n",
      "508 128 32 64\n",
      "509 48 32 128\n",
      "510 128 64 128\n",
      "511 128 32 64\n",
      "512 128 32 64\n",
      "513 48 32 128\n",
      "514 48 32 128\n",
      "515 32 128 384\n",
      "516 16 64 384\n",
      "517 64 64 192\n",
      "518 8 128 512\n",
      "519 16 256 512\n",
      "520 128 64 128\n",
      "521 16 64 384\n",
      "522 16 256 512\n",
      "523 8 32 256\n",
      "524 64 128 256\n",
      "525 8 32 256\n",
      "526 16 32 192\n",
      "527 64 128 256\n",
      "528 16 64 384\n",
      "529 64 128 256\n",
      "530 8 32 256\n",
      "531 16 256 512\n",
      "532 32 128 384\n",
      "533 32 128 384\n",
      "534 32 128 384\n",
      "535 16 256 512\n",
      "536 16 256 512\n",
      "537 128 32 64\n",
      "538 32 64 256\n",
      "539 16 256 512\n",
      "540 128 32 64\n",
      "541 128 64 128\n",
      "542 32 128 384\n",
      "543 128 64 128\n",
      "544 16 32 192\n",
      "545 48 32 128\n",
      "546 64 64 192\n",
      "547 48 32 128\n",
      "548 8 32 256\n",
      "549 32 128 384\n",
      "550 16 256 512\n",
      "551 64 128 256\n",
      "552 16 256 512\n",
      "553 64 128 256\n",
      "554 32 256 384\n",
      "555 32 64 256\n",
      "556 32 256 384\n",
      "557 16 32 192\n",
      "558 48 32 128\n",
      "559 128 32 64\n",
      "560 8 128 512\n",
      "561 32 64 256\n",
      "562 32 256 384\n",
      "563 32 256 384\n",
      "564 128 32 64\n",
      "565 16 256 512\n",
      "566 32 256 384\n",
      "567 64 64 192\n",
      "568 128 32 64\n",
      "569 32 256 384\n",
      "570 32 128 384\n",
      "571 32 128 384\n",
      "572 8 128 512\n",
      "573 32 256 384\n",
      "574 16 64 384\n",
      "575 8 32 256\n",
      "576 32 256 384\n",
      "577 16 256 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578 32 128 384\n",
      "579 16 256 512\n",
      "580 32 64 256\n",
      "581 8 32 256\n",
      "582 32 64 256\n",
      "583 32 256 384\n",
      "584 48 32 128\n",
      "585 32 256 384\n",
      "586 16 256 512\n",
      "587 16 256 512\n",
      "588 16 256 512\n",
      "589 128 64 128\n",
      "590 16 32 192\n",
      "591 16 64 384\n",
      "592 64 128 256\n",
      "593 128 64 128\n",
      "594 8 128 512\n",
      "595 32 256 384\n",
      "596 64 128 256\n",
      "597 8 32 256\n",
      "598 128 64 128\n",
      "599 8 128 512\n",
      "600 32 128 384\n",
      "601 16 32 192\n",
      "602 8 128 512\n",
      "603 16 256 512\n",
      "604 32 64 256\n",
      "605 32 256 384\n",
      "606 8 128 512\n",
      "607 16 64 384\n",
      "608 32 256 384\n",
      "609 16 32 192\n",
      "610 64 128 256\n",
      "611 64 64 192\n",
      "612 32 64 256\n",
      "613 16 256 512\n",
      "614 48 32 128\n",
      "615 32 128 384\n",
      "616 16 256 512\n",
      "617 32 256 384\n",
      "618 64 128 256\n",
      "619 32 128 384\n",
      "620 32 64 256\n",
      "621 32 128 384\n",
      "622 48 32 128\n",
      "623 64 128 256\n",
      "624 128 64 128\n",
      "625 64 128 256\n",
      "626 128 64 128\n",
      "627 32 256 384\n",
      "628 48 32 128\n",
      "629 16 256 512\n",
      "630 16 256 512\n",
      "631 8 32 256\n",
      "632 64 128 256\n",
      "633 64 64 192\n",
      "634 8 128 512\n",
      "635 48 32 128\n",
      "636 48 32 128\n",
      "637 16 256 512\n",
      "638 128 32 64\n",
      "639 32 256 384\n",
      "640 32 128 384\n",
      "641 32 256 384\n",
      "642 64 64 192\n",
      "643 32 256 384\n",
      "644 64 64 192\n",
      "645 32 128 384\n",
      "646 128 64 128\n",
      "647 8 32 256\n",
      "648 128 64 128\n",
      "649 64 64 192\n",
      "650 16 256 512\n",
      "651 32 64 256\n",
      "652 16 32 192\n",
      "653 16 256 512\n",
      "654 32 256 384\n",
      "655 128 32 64\n",
      "656 64 128 256\n",
      "657 32 256 384\n",
      "658 8 128 512\n",
      "659 128 64 128\n",
      "660 16 32 192\n",
      "661 32 128 384\n",
      "662 16 32 192\n",
      "663 8 128 512\n",
      "664 128 64 128\n",
      "665 16 256 512\n",
      "666 64 64 192\n",
      "667 64 128 256\n",
      "668 128 32 64\n",
      "669 16 256 512\n",
      "670 64 128 256\n",
      "671 16 32 192\n",
      "672 64 64 192\n",
      "673 128 32 64\n",
      "674 16 256 512\n",
      "675 64 64 192\n",
      "676 16 32 192\n",
      "677 128 64 128\n",
      "678 128 64 128\n",
      "679 8 128 512\n",
      "680 32 128 384\n",
      "681 64 128 256\n",
      "682 48 32 128\n",
      "683 128 32 64\n",
      "684 16 256 512\n",
      "685 32 64 256\n",
      "686 128 64 128\n",
      "687 32 256 384\n",
      "688 16 256 512\n",
      "689 48 32 128\n",
      "690 8 32 256\n",
      "691 64 128 256\n",
      "692 32 128 384\n",
      "693 32 256 384\n",
      "694 48 32 128\n",
      "695 32 256 384\n",
      "696 48 32 128\n",
      "697 32 64 256\n",
      "698 8 128 512\n",
      "699 128 64 128\n",
      "700 16 32 192\n",
      "701 16 32 192\n",
      "702 64 64 192\n",
      "703 48 32 128\n",
      "704 32 64 256\n",
      "705 32 64 256\n",
      "706 64 64 192\n",
      "707 16 32 192\n",
      "708 32 256 384\n",
      "709 32 256 384\n",
      "710 32 256 384\n",
      "711 128 64 128\n",
      "712 32 64 256\n",
      "713 32 256 384\n",
      "714 64 128 256\n",
      "715 64 64 192\n",
      "716 16 64 384\n",
      "717 32 64 256\n",
      "718 128 32 64\n",
      "719 48 32 128\n",
      "720 16 256 512\n",
      "721 128 32 64\n",
      "722 64 64 192\n",
      "723 8 32 256\n",
      "724 32 128 384\n",
      "725 16 256 512\n",
      "726 64 64 192\n",
      "727 8 128 512\n",
      "728 64 128 256\n",
      "729 16 256 512\n",
      "730 8 128 512\n",
      "731 8 128 512\n",
      "732 32 256 384\n",
      "733 16 256 512\n",
      "734 16 64 384\n",
      "735 32 128 384\n",
      "736 32 256 384\n",
      "737 32 128 384\n",
      "738 16 64 384\n",
      "739 32 128 384\n",
      "740 32 256 384\n",
      "741 64 64 192\n",
      "742 32 128 384\n",
      "743 32 128 384\n",
      "744 16 256 512\n",
      "745 16 256 512\n",
      "746 16 32 192\n",
      "747 16 256 512\n",
      "748 128 64 128\n",
      "749 32 128 384\n",
      "750 64 128 256\n",
      "751 32 256 384\n",
      "752 16 32 192\n",
      "753 64 64 192\n",
      "754 32 128 384\n",
      "755 8 32 256\n",
      "756 32 256 384\n",
      "757 16 256 512\n",
      "758 32 128 384\n",
      "759 8 128 512\n",
      "760 48 32 128\n",
      "761 32 128 384\n",
      "762 128 64 128\n",
      "763 8 128 512\n",
      "764 16 256 512\n",
      "765 8 128 512\n",
      "766 16 64 384\n",
      "767 128 64 128\n",
      "768 32 64 256\n",
      "769 48 32 128\n",
      "770 64 128 256\n",
      "771 32 64 256\n",
      "772 48 32 128\n",
      "773 8 32 256\n",
      "774 8 128 512\n",
      "775 32 128 384\n",
      "776 32 256 384\n",
      "777 16 64 384\n",
      "778 128 32 64\n",
      "779 8 128 512\n",
      "780 8 32 256\n",
      "781 48 32 128\n",
      "782 32 256 384\n",
      "783 32 256 384\n",
      "784 128 32 64\n",
      "785 8 128 512\n",
      "786 64 64 192\n",
      "787 64 128 256\n",
      "788 32 64 256\n",
      "789 64 128 256\n",
      "790 128 32 64\n",
      "791 128 32 64\n",
      "792 64 128 256\n",
      "793 128 32 64\n",
      "794 64 128 256\n",
      "795 8 32 256\n",
      "796 16 256 512\n",
      "797 8 128 512\n",
      "798 32 256 384\n",
      "799 64 128 256\n",
      "800 64 64 192\n",
      "801 8 128 512\n",
      "802 16 256 512\n",
      "803 16 32 192\n",
      "804 64 128 256\n",
      "805 64 64 192\n",
      "806 48 32 128\n",
      "807 32 128 384\n",
      "808 128 64 128\n",
      "809 64 64 192\n",
      "810 16 256 512\n",
      "811 64 64 192\n",
      "812 48 32 128\n",
      "813 16 256 512\n",
      "814 8 128 512\n",
      "815 16 256 512\n",
      "816 48 32 128\n",
      "817 128 32 64\n",
      "818 128 64 128\n",
      "819 16 32 192\n",
      "820 16 256 512\n",
      "821 32 128 384\n",
      "822 32 64 256\n",
      "823 64 128 256\n",
      "824 64 128 256\n",
      "825 32 256 384\n",
      "826 16 32 192\n",
      "827 8 32 256\n",
      "828 8 32 256\n",
      "829 128 64 128\n",
      "830 48 32 128\n",
      "831 16 256 512\n",
      "832 32 256 384\n",
      "833 32 128 384\n",
      "834 32 64 256\n",
      "835 32 256 384\n",
      "836 48 32 128\n",
      "837 32 256 384\n",
      "838 128 64 128\n",
      "839 16 256 512\n",
      "840 16 256 512\n",
      "841 64 64 192\n",
      "842 64 128 256\n",
      "843 32 128 384\n",
      "844 16 64 384\n",
      "845 8 128 512\n",
      "846 32 128 384\n",
      "847 32 128 384\n",
      "848 32 256 384\n",
      "849 16 256 512\n",
      "850 48 32 128\n",
      "851 32 64 256\n",
      "852 32 256 384\n",
      "853 64 128 256\n",
      "854 8 128 512\n",
      "855 8 32 256\n",
      "856 64 64 192\n",
      "857 32 128 384\n",
      "858 32 256 384\n",
      "859 64 64 192\n",
      "860 32 256 384\n",
      "861 16 256 512\n",
      "862 16 64 384\n",
      "863 32 64 256\n",
      "864 32 128 384\n",
      "865 1 32 256\n",
      "866 16 64 384\n",
      "867 16 256 512\n",
      "868 32 128 384\n",
      "869 32 256 384\n",
      "870 16 256 512\n",
      "871 128 32 64\n",
      "872 128 32 64\n",
      "873 16 64 384\n",
      "874 32 128 384\n",
      "875 16 256 512\n",
      "876 128 64 128\n",
      "877 64 64 192\n",
      "878 64 128 256\n",
      "879 32 64 256\n",
      "880 32 64 256\n",
      "881 64 64 192\n",
      "882 16 256 512\n",
      "883 32 256 384\n",
      "884 16 64 384\n",
      "885 32 256 384\n",
      "886 32 256 384\n",
      "887 32 256 384\n",
      "888 64 128 256\n",
      "889 16 256 512\n",
      "890 48 32 128\n",
      "891 128 64 128\n",
      "892 32 128 384\n",
      "893 32 64 256\n",
      "894 48 32 128\n",
      "895 32 256 384\n",
      "896 64 128 256\n",
      "897 64 64 192\n",
      "898 32 64 256\n",
      "899 48 32 128\n",
      "900 64 64 192\n",
      "901 32 256 384\n",
      "902 16 256 512\n",
      "903 32 64 256\n",
      "904 32 128 384\n",
      "905 32 256 384\n",
      "906 16 64 384\n",
      "907 64 128 256\n",
      "908 32 128 384\n",
      "909 16 256 512\n",
      "910 32 256 384\n",
      "911 16 256 512\n",
      "912 64 64 192\n",
      "913 32 256 384\n",
      "914 8 128 512\n",
      "915 32 64 256\n",
      "916 16 256 512\n",
      "917 128 32 64\n",
      "918 128 32 64\n",
      "919 32 128 384\n",
      "920 48 32 128\n",
      "921 32 128 384\n",
      "922 48 32 128\n",
      "923 32 128 384\n",
      "924 32 256 384\n",
      "925 64 128 256\n",
      "926 16 32 192\n",
      "927 48 32 128\n",
      "928 8 32 256\n",
      "929 64 128 256\n",
      "930 32 64 256\n",
      "931 16 32 192\n",
      "932 32 64 256\n",
      "933 32 256 384\n",
      "934 128 32 64\n",
      "935 64 128 256\n",
      "936 32 256 384\n",
      "937 16 256 512\n",
      "938 128 64 128\n",
      "939 128 64 128\n",
      "940 32 256 384\n",
      "941 16 32 192\n",
      "942 64 128 256\n",
      "943 48 32 128\n",
      "944 128 32 64\n",
      "945 8 128 512\n",
      "946 16 64 384\n",
      "947 8 128 512\n",
      "948 8 32 256\n",
      "949 32 128 384\n",
      "950 32 256 384\n",
      "951 16 32 192\n",
      "952 16 256 512\n",
      "953 128 64 128\n",
      "954 16 256 512\n",
      "955 32 256 384\n",
      "956 16 64 384\n",
      "957 32 128 384\n",
      "958 32 256 384\n",
      "959 16 64 384\n",
      "960 16 256 512\n",
      "961 8 128 512\n",
      "962 128 64 128\n",
      "963 32 64 256\n",
      "964 48 32 128\n",
      "965 16 256 512\n",
      "966 128 64 128\n",
      "967 16 256 512\n",
      "968 16 256 512\n",
      "969 48 32 128\n",
      "970 64 128 256\n",
      "971 16 64 384\n",
      "972 128 64 128\n",
      "973 8 128 512\n",
      "974 16 256 512\n",
      "975 16 256 512\n",
      "976 64 128 256\n",
      "977 16 256 512\n",
      "978 16 256 512\n",
      "979 8 128 512\n",
      "980 128 32 64\n",
      "981 16 64 384\n",
      "982 16 32 192\n",
      "983 64 128 256\n",
      "984 16 64 384\n",
      "985 16 256 512\n",
      "986 32 64 256\n",
      "987 64 64 192\n",
      "988 16 256 512\n",
      "989 64 128 256\n",
      "990 128 64 128\n",
      "991 32 128 384\n",
      "992 8 32 256\n",
      "993 128 32 64\n",
      "994 32 128 384\n",
      "995 128 32 64\n",
      "996 16 256 512\n",
      "997 32 256 384\n",
      "998 32 128 384\n",
      "999 8 128 512\n",
      "1000 32 256 384\n",
      "1001 48 32 128\n",
      "1002 16 256 512\n",
      "1003 8 128 512\n",
      "1004 8 32 256\n",
      "1005 64 64 192\n",
      "1006 48 32 128\n",
      "1007 32 128 384\n",
      "1008 16 32 192\n",
      "1009 32 256 384\n",
      "1010 64 64 192\n",
      "1011 32 128 384\n",
      "1012 8 32 256\n",
      "1013 64 128 256\n",
      "1014 32 128 384\n",
      "1015 16 32 192\n",
      "1016 48 32 128\n",
      "1017 16 64 384\n",
      "1018 8 32 256\n",
      "1019 32 64 256\n",
      "1020 48 32 128\n",
      "1021 64 128 256\n",
      "1022 16 32 192\n",
      "1023 32 128 384\n",
      "1024 32 256 384\n",
      "1025 64 128 256\n",
      "1026 16 64 384\n",
      "1027 128 32 64\n",
      "1028 64 128 256\n",
      "1029 128 32 64\n",
      "1030 8 32 256\n",
      "1031 32 64 256\n",
      "1032 32 256 384\n",
      "1033 128 64 128\n",
      "1034 64 128 256\n",
      "1035 32 256 384\n",
      "1036 64 128 256\n",
      "1037 128 64 128\n",
      "1038 128 64 128\n",
      "1039 128 64 128\n",
      "1040 128 32 64\n",
      "1041 16 64 384\n",
      "1042 128 32 64\n",
      "1043 128 32 64\n",
      "1044 8 128 512\n",
      "1045 16 32 192\n",
      "1046 16 64 384\n",
      "1047 8 128 512\n",
      "1048 16 64 384\n",
      "1049 128 64 128\n",
      "1050 48 32 128\n",
      "1051 8 128 512\n",
      "1052 16 32 192\n",
      "1053 128 64 128\n",
      "1054 64 128 256\n",
      "1055 16 256 512\n",
      "1056 16 256 512\n",
      "1057 48 32 128\n",
      "1058 48 32 128\n",
      "1059 8 128 512\n",
      "1060 16 32 192\n",
      "1061 8 128 512\n",
      "1062 32 128 384\n",
      "1063 32 64 256\n",
      "1064 32 256 384\n",
      "1065 16 32 192\n",
      "1066 32 128 384\n",
      "1067 16 256 512\n",
      "1068 16 256 512\n",
      "1069 128 32 64\n",
      "1070 16 64 384\n",
      "1071 48 32 128\n",
      "1072 16 256 512\n",
      "1073 16 256 512\n",
      "1074 48 32 128\n",
      "1075 32 256 384\n",
      "1076 8 128 512\n",
      "1077 8 128 512\n",
      "1078 48 32 128\n",
      "1079 16 32 192\n",
      "1080 16 256 512\n",
      "1081 16 64 384\n",
      "1082 32 128 384\n",
      "1083 8 32 256\n",
      "1084 64 128 256\n",
      "1085 32 128 384\n",
      "1086 16 32 192\n",
      "1087 8 32 256\n",
      "1088 8 32 256\n",
      "1089 16 256 512\n",
      "1090 128 32 64\n",
      "1091 16 256 512\n",
      "1092 16 32 192\n",
      "1093 16 32 192\n",
      "1094 32 64 256\n",
      "1095 128 32 64\n",
      "1096 8 32 256\n",
      "1097 48 32 128\n",
      "1098 128 32 64\n",
      "1099 32 128 384\n",
      "1100 128 64 128\n",
      "1101 48 32 128\n",
      "1102 16 256 512\n",
      "1103 16 32 192\n",
      "1104 32 256 384\n",
      "1105 48 32 128\n",
      "1106 16 32 192\n",
      "1107 16 256 512\n",
      "1108 48 32 128\n",
      "1109 16 32 192\n",
      "1110 16 256 512\n",
      "1111 16 256 512\n",
      "1112 16 32 192\n",
      "1113 32 256 384\n",
      "1114 64 128 256\n",
      "1115 8 128 512\n",
      "1116 128 64 128\n",
      "1117 64 64 192\n",
      "1118 16 256 512\n",
      "1119 32 256 384\n",
      "1120 16 256 512\n",
      "1121 128 64 128\n",
      "1122 8 32 256\n",
      "1123 128 32 64\n",
      "1124 32 256 384\n",
      "1125 16 64 384\n",
      "1126 32 64 256\n",
      "1127 64 64 192\n",
      "1128 48 32 128\n",
      "1129 16 256 512\n",
      "1130 32 64 256\n",
      "1131 8 32 256\n",
      "1132 64 128 256\n",
      "1133 64 128 256\n",
      "1134 16 256 512\n",
      "1135 48 32 128\n",
      "1136 8 32 256\n",
      "1137 8 32 256\n",
      "1138 128 32 64\n",
      "1139 8 128 512\n",
      "1140 32 256 384\n",
      "1141 32 128 384\n",
      "1142 16 256 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1143 16 64 384\n",
      "1144 32 256 384\n",
      "1145 32 256 384\n",
      "1146 48 32 128\n",
      "1147 32 256 384\n",
      "1148 16 256 512\n",
      "1149 64 64 192\n",
      "1150 128 32 64\n",
      "1151 64 128 256\n",
      "1152 16 256 512\n",
      "1153 16 256 512\n",
      "1154 32 128 384\n",
      "1155 16 32 192\n",
      "1156 16 32 192\n",
      "1157 16 32 192\n",
      "1158 64 64 192\n",
      "1159 8 32 256\n",
      "1160 32 256 384\n",
      "1161 64 128 256\n",
      "1162 64 64 192\n",
      "1163 16 32 192\n",
      "1164 8 128 512\n",
      "1165 16 256 512\n",
      "1166 8 128 512\n",
      "1167 32 256 384\n",
      "1168 8 32 256\n",
      "1169 16 256 512\n",
      "1170 16 256 512\n",
      "1171 16 64 384\n",
      "1172 16 256 512\n",
      "1173 64 128 256\n",
      "1174 16 64 384\n",
      "1175 16 256 512\n",
      "1176 16 256 512\n",
      "1177 64 128 256\n",
      "1178 16 256 512\n",
      "1179 16 256 512\n",
      "1180 16 64 384\n",
      "1181 48 32 128\n",
      "1182 32 64 256\n",
      "1183 32 64 256\n",
      "1184 32 128 384\n",
      "1185 16 256 512\n",
      "1186 8 128 512\n",
      "1187 32 128 384\n",
      "1188 32 256 384\n",
      "1189 16 32 192\n",
      "1190 8 32 256\n",
      "1191 32 64 256\n",
      "1192 48 32 128\n"
     ]
    }
   ],
   "source": [
    "count=[]\n",
    "for index,batch in enumerate(loader):\n",
    "    count.append(dict(batch_size=len(batch['input']),res=batch['res'],pic_size=batch['input'].shape[2]))\n",
    "    print(index,len(batch['wh']),batch['res'],batch['input'].shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_pic=dict(pic_64=0,pic_128=0,pic_192=0,pic_256=0,pic_384=0,pic_512=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cou in count:\n",
    "    count_pic['pic_'+str(cou['pic_size'])]+=cou['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_res=dict(res_64=0,res_128=0,res_32=0,res_256=0)\n",
    "for cou in count:\n",
    "    count_res['res_'+str(cou['res'])]+=cou['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'res_64': 17888, 'res_128': 9984, 'res_32': 16489, 'res_256': 7056}\n"
     ]
    }
   ],
   "source": [
    "print(count_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pic_64': 10752, 'pic_128': 13824, 'pic_192': 5904, 'pic_256': 9113, 'pic_384': 8320, 'pic_512': 3504}\n"
     ]
    }
   ],
   "source": [
    "print(count_pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:3.645(6.042) time:1.609 epoch0/100: 100%|██████████| 1193/1193 [26:32<00:00,  1.52s/it]     \n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 loss:6.042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:3.072(3.492) time:1.576 epoch1/100: 100%|██████████| 1193/1193 [26:26<00:00,  1.40s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 loss:3.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:2.957(3.314) time:1.211 epoch2/100: 100%|██████████| 1193/1193 [26:24<00:00,  1.20s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 loss:3.314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:3.529(3.140) time:1.339 epoch3/100: 100%|██████████| 1193/1193 [26:17<00:00,  1.43s/it] \n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 loss:3.140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:3.487(3.026) time:1.518 epoch4/100: 100%|██████████| 1193/1193 [26:26<00:00,  1.56s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 loss:3.026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:3.332(3.044) time:1.350 epoch5/100: 100%|██████████| 1193/1193 [26:53<00:00,  1.44s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 loss:3.044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:2.737(2.964) time:1.521 epoch6/100: 100%|██████████| 1193/1193 [27:04<00:00,  1.34s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 loss:2.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:3.915(2.937) time:0.969 epoch7/100: 100%|██████████| 1193/1193 [26:59<00:00,  1.56s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 loss:2.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:2.609(2.825) time:1.354 epoch8/100: 100%|██████████| 1193/1193 [27:06<00:00,  1.29s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 loss:2.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:3.149(2.785) time:1.646 epoch9/100: 100%|██████████| 1193/1193 [27:02<00:00,  1.40s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 loss:2.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.340(2.711) time:0.980 epoch10/100: 100%|██████████| 1193/1193 [27:28<00:00,  1.15s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 loss:2.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:1.753(2.677) time:0.815 epoch11/100: 100%|██████████| 1193/1193 [26:45<00:00,  1.17s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 loss:2.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:2.451(2.627) time:1.251 epoch12/100: 100%|██████████| 1193/1193 [27:32<00:00,  1.43s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 loss:2.627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:2.112(2.578) time:1.736 epoch13/100: 100%|██████████| 1193/1193 [26:42<00:00,  1.47s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 loss:2.578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.461(2.559) time:0.982 epoch14/100: 100%|██████████| 1193/1193 [27:12<00:00,  1.19s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 loss:2.559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:2.932(2.490) time:0.963 epoch15/100: 100%|██████████| 1193/1193 [27:12<00:00,  1.06s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 loss:2.490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:1.591(2.434) time:1.994 epoch16/100: 100%|██████████| 1193/1193 [27:20<00:00,  1.39s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 loss:2.434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:2.500(2.365) time:1.600 epoch17/100: 100%|██████████| 1193/1193 [27:19<00:00,  1.47s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 loss:2.365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:2.856(2.352) time:0.965 epoch18/100: 100%|██████████| 1193/1193 [27:22<00:00,  1.44s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 loss:2.352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:2.144(2.318) time:2.233 epoch19/100: 100%|██████████| 1193/1193 [27:04<00:00,  1.58s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 loss:2.318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:2.070(2.217) time:1.002 epoch20/100: 100%|██████████| 1193/1193 [27:17<00:00,  1.41s/it] \n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 loss:2.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:1.433(2.152) time:1.049 epoch21/100: 100%|██████████| 1193/1193 [27:19<00:00,  1.18s/it] \n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 loss:2.152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.397(2.135) time:0.965 epoch22/100: 100%|██████████| 1193/1193 [27:04<00:00,  1.32s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 loss:2.135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:2.223(2.021) time:1.274 epoch23/100: 100%|██████████| 1193/1193 [27:15<00:00,  1.43s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 loss:2.021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:2.229(2.054) time:2.143 epoch24/100: 100%|██████████| 1193/1193 [27:29<00:00,  1.62s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 loss:2.054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:1.499(1.982) time:1.003 epoch25/100: 100%|██████████| 1193/1193 [27:11<00:00,  1.29s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 loss:1.982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:2.107(1.948) time:1.520 epoch26/100: 100%|██████████| 1193/1193 [27:14<00:00,  1.26s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 loss:1.948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.438(1.899) time:0.955 epoch27/100: 100%|██████████| 1193/1193 [27:04<00:00,  1.34s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 loss:1.899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:3.658(1.851) time:0.952 epoch28/100: 100%|██████████| 1193/1193 [27:10<00:00,  1.37s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 loss:1.851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:1.642(1.768) time:0.989 epoch29/100: 100%|██████████| 1193/1193 [26:48<00:00,  1.14s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 loss:1.768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:2.675(1.881) time:1.469 epoch30/100: 100%|██████████| 1193/1193 [26:28<00:00,  1.45s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 loss:1.881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:2.292(1.798) time:1.324 epoch31/100: 100%|██████████| 1193/1193 [26:12<00:00,  1.15s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 loss:1.798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:2.020(1.810) time:1.593 epoch32/100: 100%|██████████| 1193/1193 [26:20<00:00,  1.35s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 loss:1.810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.290(1.681) time:0.945 epoch33/100: 100%|██████████| 1193/1193 [26:35<00:00,  1.16s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 loss:1.681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:1.938(1.635) time:1.445 epoch34/100: 100%|██████████| 1193/1193 [26:29<00:00,  1.40s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 loss:1.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.597(1.554) time:0.765 epoch35/100: 100%|██████████| 1193/1193 [26:35<00:00,  1.16s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 loss:1.554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:1.151(1.614) time:1.331 epoch36/100: 100%|██████████| 1193/1193 [26:20<00:00,  1.31s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 loss:1.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:1.672(1.514) time:1.587 epoch37/100: 100%|██████████| 1193/1193 [26:30<00:00,  1.40s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 loss:1.514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.811(1.421) time:0.979 epoch38/100: 100%|██████████| 1193/1193 [26:25<00:00,  1.35s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 loss:1.421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.722(1.419) time:0.947 epoch39/100: 100%|██████████| 1193/1193 [26:29<00:00,  1.08s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 loss:1.419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:1.562(1.223) time:1.327 epoch40/100: 100%|██████████| 1193/1193 [26:37<00:00,  1.49s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 loss:1.223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.731(1.118) time:1.698 epoch41/100: 100%|██████████| 1193/1193 [26:12<00:00,  1.25s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 loss:1.118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.755(1.109) time:1.747 epoch42/100: 100%|██████████| 1193/1193 [26:39<00:00,  1.53s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 loss:1.109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.920(1.050) time:2.105 epoch43/100: 100%|██████████| 1193/1193 [26:21<00:00,  1.64s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 loss:1.050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.694(1.039) time:1.622 epoch44/100: 100%|██████████| 1193/1193 [26:41<00:00,  1.47s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 loss:1.039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:1.535(0.994) time:0.967 epoch45/100: 100%|██████████| 1193/1193 [26:30<00:00,  1.29s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 loss:0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.852(0.968) time:0.972 epoch46/100: 100%|██████████| 1193/1193 [26:29<00:00,  1.15s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 loss:0.968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:1.067(1.001) time:1.782 epoch47/100: 100%|██████████| 1193/1193 [26:23<00:00,  1.39s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 loss:1.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:1.798(0.934) time:2.277 epoch48/100: 100%|██████████| 1193/1193 [26:33<00:00,  1.45s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 loss:0.934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.663(0.902) time:2.069 epoch49/100: 100%|██████████| 1193/1193 [26:32<00:00,  1.54s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 loss:0.902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.733(0.916) time:1.455 epoch50/100: 100%|██████████| 1193/1193 [26:41<00:00,  1.31s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:50 loss:0.916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.063(0.851) time:0.762 epoch51/100: 100%|██████████| 1193/1193 [26:28<00:00,  1.19s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:51 loss:0.851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.481(0.862) time:1.720 epoch52/100: 100%|██████████| 1193/1193 [26:12<00:00,  1.49s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:52 loss:0.862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.484(0.801) time:1.707 epoch53/100: 100%|██████████| 1193/1193 [26:30<00:00,  1.38s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:53 loss:0.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.782(0.803) time:1.357 epoch54/100: 100%|██████████| 1193/1193 [26:31<00:00,  1.48s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:54 loss:0.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:1.033(0.768) time:1.192 epoch55/100: 100%|██████████| 1193/1193 [26:32<00:00,  1.10s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:55 loss:0.768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.998(0.803) time:1.454 epoch56/100: 100%|██████████| 1193/1193 [26:58<00:00,  1.28s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:56 loss:0.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.718(0.766) time:1.730 epoch57/100: 100%|██████████| 1193/1193 [26:37<00:00,  1.24s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:57 loss:0.766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.936(0.732) time:2.101 epoch58/100: 100%|██████████| 1193/1193 [26:58<00:00,  1.50s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:58 loss:0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.428(0.734) time:1.747 epoch59/100: 100%|██████████| 1193/1193 [26:48<00:00,  1.37s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:59 loss:0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.611(0.648) time:1.777 epoch60/100: 100%|██████████| 1193/1193 [26:42<00:00,  1.37s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:60 loss:0.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.545(0.607) time:1.460 epoch61/100: 100%|██████████| 1193/1193 [27:16<00:00,  1.58s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:61 loss:0.607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.346(0.626) time:1.540 epoch62/100: 100%|██████████| 1193/1193 [26:57<00:00,  1.53s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:62 loss:0.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.261(0.560) time:1.798 epoch63/100: 100%|██████████| 1193/1193 [26:57<00:00,  1.57s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:63 loss:0.560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.333(0.556) time:1.643 epoch64/100: 100%|██████████| 1193/1193 [27:07<00:00,  1.23s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:64 loss:0.556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.337(0.542) time:1.522 epoch65/100: 100%|██████████| 1193/1193 [27:07<00:00,  1.30s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:65 loss:0.542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.021(0.529) time:0.777 epoch66/100: 100%|██████████| 1193/1193 [27:01<00:00,  1.21s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:66 loss:0.529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.092(0.536) time:0.783 epoch67/100: 100%|██████████| 1193/1193 [27:03<00:00,  1.07s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:67 loss:0.536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.330(0.514) time:1.655 epoch68/100: 100%|██████████| 1193/1193 [27:22<00:00,  1.58s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:68 loss:0.514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.518(0.534) time:2.176 epoch69/100: 100%|██████████| 1193/1193 [26:51<00:00,  1.53s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:69 loss:0.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.164(0.490) time:1.484 epoch70/100: 100%|██████████| 1193/1193 [27:08<00:00,  1.35s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:70 loss:0.490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.185(0.468) time:0.758 epoch71/100: 100%|██████████| 1193/1193 [26:37<00:00,  1.01it/s]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:71 loss:0.468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.431(0.461) time:1.741 epoch72/100: 100%|██████████| 1193/1193 [27:05<00:00,  1.34s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:72 loss:0.461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:3.350(0.465) time:0.263 epoch73/100: 100%|██████████| 1193/1193 [26:41<00:00,  1.00s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:73 loss:0.465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.503(0.454) time:0.980 epoch74/100: 100%|██████████| 1193/1193 [26:50<00:00,  1.22s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:74 loss:0.454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.039(0.464) time:0.982 epoch75/100: 100%|██████████| 1193/1193 [26:44<00:00,  1.20s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:75 loss:0.464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.615(0.458) time:2.106 epoch76/100: 100%|██████████| 1193/1193 [26:48<00:00,  1.51s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:76 loss:0.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.678(0.457) time:0.959 epoch77/100: 100%|██████████| 1193/1193 [26:46<00:00,  1.30s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:77 loss:0.457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.702(0.437) time:0.957 epoch78/100: 100%|██████████| 1193/1193 [27:05<00:00,  1.13s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:78 loss:0.437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.345(0.426) time:1.651 epoch79/100: 100%|██████████| 1193/1193 [26:35<00:00,  1.58s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:79 loss:0.426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.242(0.400) time:1.340 epoch80/100: 100%|██████████| 1193/1193 [26:24<00:00,  1.32s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:80 loss:0.400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.199(0.409) time:0.966 epoch81/100: 100%|██████████| 1193/1193 [26:26<00:00,  1.06s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:81 loss:0.409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.179(0.392) time:1.339 epoch82/100: 100%|██████████| 1193/1193 [26:21<00:00,  1.55s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:82 loss:0.392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.305(0.360) time:1.726 epoch83/100: 100%|██████████| 1193/1193 [26:33<00:00,  1.58s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:83 loss:0.360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.279(0.355) time:1.630 epoch84/100: 100%|██████████| 1193/1193 [26:23<00:00,  1.61s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:84 loss:0.355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.048(0.382) time:0.949 epoch85/100: 100%|██████████| 1193/1193 [26:21<00:00,  1.18s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:85 loss:0.382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.173(0.392) time:1.477 epoch86/100: 100%|██████████| 1193/1193 [27:06<00:00,  1.43s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:86 loss:0.392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.009(0.374) time:0.756 epoch87/100: 100%|██████████| 1193/1193 [26:08<00:00,  1.12s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:87 loss:0.374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.003(0.351) time:0.962 epoch88/100: 100%|██████████| 1193/1193 [26:22<00:00,  1.08s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:88 loss:0.351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.291(0.371) time:0.990 epoch89/100: 100%|██████████| 1193/1193 [26:07<00:00,  1.24s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:89 loss:0.371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.121(0.371) time:0.982 epoch90/100: 100%|██████████| 1193/1193 [26:21<00:00,  1.12s/it] \n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:90 loss:0.371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.238(0.351) time:1.493 epoch91/100: 100%|██████████| 1193/1193 [26:43<00:00,  1.41s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:91 loss:0.351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:1.165(0.381) time:2.124 epoch92/100: 100%|██████████| 1193/1193 [26:27<00:00,  1.32s/it] \n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:92 loss:0.381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.128(0.344) time:0.965 epoch93/100: 100%|██████████| 1193/1193 [26:18<00:00,  1.21s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:93 loss:0.344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.047(0.324) time:0.954 epoch94/100: 100%|██████████| 1193/1193 [26:22<00:00,  1.24s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:94 loss:0.324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.414(0.336) time:1.460 epoch95/100: 100%|██████████| 1193/1193 [26:14<00:00,  1.40s/it]\n",
      "  0%|          | 0/1193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:95 loss:0.336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.288(0.338) time:1.468 epoch96/100:  60%|██████    | 717/1193 [15:56<10:44,  1.35s/it]"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FocalLoss, self).__init__()\n",
    "#         self.loss_func=torch.nn.MSELoss(reduction='none')\n",
    "        self.neg_loss = self._neg_loss\n",
    "    def forward(self,output,hm):   \n",
    "        output=self.sigmoid(output)\n",
    "        loss=self.neg_loss(output,hm)\n",
    "        return loss\n",
    "    def sigmoid(self,x):\n",
    "        y = torch.clamp(x.sigmoid_(), min=1e-4, max=1-1e-4)\n",
    "        return y\n",
    "    def _neg_loss(self,pred, gt):\n",
    "        pos_inds = gt.eq(1).float()\n",
    "        neg_inds = gt.lt(1).float()\n",
    "\n",
    "        neg_weights = torch.pow(1 - gt, 4)\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        pos_loss = torch.log(pred) * torch.pow(1 - pred, 2) * pos_inds\n",
    "        neg_loss = torch.log(1 - pred) * torch.pow(pred, 2) * neg_weights * neg_inds\n",
    "\n",
    "        num_pos  = pos_inds.float().sum()\n",
    "        pos_loss = pos_loss.sum()\n",
    "        neg_loss = neg_loss.sum()\n",
    "\n",
    "        if num_pos == 0:\n",
    "            loss = loss - neg_loss\n",
    "        else:\n",
    "            loss = loss - (pos_loss + neg_loss) / num_pos\n",
    "        return loss\n",
    "    \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "acc_log=AverageMeter()\n",
    "loss_log=AverageMeter()\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "cuda_device=1\n",
    "model=resnet_mr(num_classes=20)\n",
    "model.cuda(cuda_device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 40, 60, 80,100], gamma=0.5)\n",
    "# params=torch.load('params.pth')\n",
    "# model.load_state_dict(params)\n",
    "end=time.time()\n",
    "loss_func=FocalLoss()\n",
    "\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).view(1,1,3)\n",
    "std  = torch.tensor([0.229, 0.224, 0.225]).view(1,1,3)\n",
    "def draw_pic_hm(data,batch_heatmap):\n",
    "    classes=[\"aeroplane\", \"bicycle\", \"bird\", \"boat\",\"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \n",
    "\"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \n",
    "\"train\", \"tvmonitor\"]\n",
    "    for pic,heatmap in zip(data,batch_heatmap):\n",
    "        plt.imshow(pic.permute(1,2,0)*std+mean)\n",
    "        plt.show()\n",
    "        plt.figure(figsize=[20,20])\n",
    "        for index,hm in enumerate(heatmap):\n",
    "            plt.subplot(4,5,index+1)\n",
    "            plt.title(classes[index])\n",
    "            plt.imshow(hm)\n",
    "        plt.show()\n",
    "def adjust_learning_rate(optimizer, batch_size,epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = 0.01*(batch_size/256)*((0.5)**(epoch//20))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "for e in range(0,100):\n",
    "    loss_log.reset()\n",
    "    scheduler.step()\n",
    "    loader=tqdm(loader)\n",
    "    for index,((data,hm),l) in enumerate(loader):\n",
    "#         print(data.shape,hm.shape)\n",
    "        num=int(math.log(l//32,2))+1\n",
    "        batch_size=len(data)\n",
    "        data=data.cuda(cuda_device,non_blocking=True)\n",
    "        hm=hm.cuda(cuda_device,non_blocking=True)\n",
    "#         bg=bg.cuda(cuda_device)\n",
    "#         clsid=clsid.cuda(cuda_device)\n",
    "        output=model(data,num,True)\n",
    "#         draw_pic_hm(data.cpu(),hm.cpu())\n",
    "    #         print(output.shape,l)\n",
    "        loss=loss_func(output,hm)\n",
    "#         print(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        adjust_learning_rate(optimizer,batch_size,e)\n",
    "        optimizer.step()\n",
    "        loss_log.update(loss.item())\n",
    "        loader.set_description('loss:%.3f(%.3f) time:%.3f epoch%d/%d'%(loss_log.val,loss_log.avg,time.time()-end,e,100))\n",
    "        end=time.time()\n",
    "    print('epoch:%d loss:%.3f'%(e,loss_log.avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loss%d'%(4.5))\n",
    "print(loss_log.avg)\n",
    "input('s')\n",
    "torch.save(model.state_dict(),'params_focal_lossss.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "cuda_device=1\n",
    "end=time.time()\n",
    "loss_func=FocalLoss()\n",
    "loader=tqdm(loader)\n",
    "model.eval()\n",
    "classes=[\"aeroplane\", \"bicycle\", \"bird\", \"boat\",\"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \n",
    "\"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \n",
    "\"train\", \"tvmonitor\"]\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).view(1,1,3)\n",
    "std  = torch.tensor([0.229, 0.224, 0.225]).view(1,1,3)\n",
    "def draw_pic_hm(data,batch_heatmap,hms):\n",
    "    classes=[\"aeroplane\", \"bicycle\", \"bird\", \"boat\",\"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \n",
    "\"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \n",
    "\"train\", \"tvmonitor\"]\n",
    "#     data=data.numpy()[...,::-1]\n",
    "    for pic,heatmap,labels_hm in zip(data,batch_heatmap,hms):\n",
    "        plt.imshow(((pic.permute((1,2,0))*std+mean)*255).int())\n",
    "        plt.show()\n",
    "#         plt.figure(figsize=[20,20])\n",
    "        plt.subplot(1,2,1)\n",
    "        for index,(out_hm) in enumerate(heatmap):\n",
    "            plt.subplot(4,5,index+1)\n",
    "            plt.title(classes[index])\n",
    "            plt.imshow(out_hm)\n",
    "        plt.show()\n",
    "        plt.subplot(1,2,2)\n",
    "        for index,(label_hm) in enumerate(labels_hm):\n",
    "            plt.subplot(4,5,index+1)\n",
    "            plt.title(classes[index])\n",
    "            plt.imshow(label_hm)\n",
    "        plt.show()\n",
    "with torch.no_grad():\n",
    "    loss_log.reset()\n",
    "    for index,((data,hm),l) in enumerate(loader):\n",
    "        num=int(math.log(l//32,2))+1\n",
    "        data=data.cuda(cuda_device)\n",
    "        hm=hm.cuda(cuda_device)\n",
    "        output=model(data,num,False)\n",
    "        draw_pic_hm(data.cpu(),loss_func.sigmoid(output.cpu()),hm.cpu())\n",
    "        input('s')\n",
    "        loss=loss_func(output,hm)\n",
    "        loss_log.update(loss.item())        \n",
    "        loader.set_description('loss:%.3f(%.3f) time:%.3f batch%d/%d'%(loss_log.val,loss_log.avg,time.time()-end,index,len(loader)))\n",
    "        input('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'params_9.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('epoch:%d loss:%.3f'%(e,loss_log.avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor(range(20))\n",
    "a=a.view(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.float().sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin loss=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,(i,j,k,z) in enumerate(loader):\n",
    "    print(i.shape,j.shape,k.shape,z)\n",
    "    a=input('s')\n",
    "    pass\n",
    "#     print(index,i.shape,j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,2,3,4]\n",
    "b=a.copy()\n",
    "b[1]=4\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_8=0\n",
    "count_16=0\n",
    "count_32=0\n",
    "count_64=0\n",
    "count_128=0\n",
    "count_256=0\n",
    "count_513=0\n",
    "count_40=0\n",
    "count_80=0\n",
    "count_160=0\n",
    "count_pics=0\n",
    "count_objs=0\n",
    "count_40_64=0\n",
    "count_40_128=0\n",
    "count_40_160=0\n",
    "count_40_256=0\n",
    "count_40_max=0\n",
    "count_80_128=0\n",
    "count_80_160=0\n",
    "count_80_256=0\n",
    "count_80_max=0\n",
    "count_160_256=0\n",
    "count_160_384=0\n",
    "count_160_512=0\n",
    "count_160_max=0\n",
    "for i in a:\n",
    "    count_pics+=1\n",
    "    whs=i['wh']\n",
    "    reg=i['reg_mask']\n",
    "    for x in reg:\n",
    "        if x==1:\n",
    "            count_objs+=1\n",
    "    for w,h in whs:\n",
    "        w,h=w*4,h*4\n",
    "        if w==0 and h==0:\n",
    "            break\n",
    "        if min(w,h)<=40:\n",
    "            count_40+=1\n",
    "            if max(w,h)<=64:\n",
    "                count_40_64+=1\n",
    "            elif max(w,h)<=128:\n",
    "                count_40_128+=1\n",
    "            elif max(w,h)<=160:\n",
    "                count_40_160+=1\n",
    "            elif max(w,h)<=256:\n",
    "                count_40_256+=1\n",
    "            else:\n",
    "                count_40_max+=1\n",
    "        elif min(w,h)<=80:\n",
    "            count_80+=1\n",
    "            if max(w,h)<=128:\n",
    "                count_80_128+=1\n",
    "            elif max(w,h)<=160:\n",
    "                count_80_160+=1\n",
    "            elif max(w,h)<=256:\n",
    "                count_80_256+=1\n",
    "            else:\n",
    "                count_80_max+=1\n",
    "        elif min(w,h)<=160:\n",
    "            count_160+=1\n",
    "            if max(w,h)<=256:\n",
    "                count_160_256+=1\n",
    "            elif max(w,h)<=384:\n",
    "                count_160_384+=1\n",
    "            elif max(w,h)<=512:\n",
    "                count_160_512+=1\n",
    "            else:\n",
    "                count_160_max+=1\n",
    "        else:\n",
    "            count_513+=1\n",
    "# print(count_small,count_middle,count_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_40,count_80,count_160,count_513)\n",
    "print(count_40_64,count_40_128,count_40_160,count_40_256,count_40_max)\n",
    "print(count_80_128,count_80_160,count_80_256,count_80_max)\n",
    "print(count_160_256,count_160_384,count_160_512,count_160_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(a)\n",
    "plt.imshow((a[3]['input'].transpose(1,2,0)*a.std+a.mean)[...,::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(i):\n",
    "    pics=a[i]['hm']\n",
    "    input_pic=a[i]['input']\n",
    "    plt.imshow((a[i]['input'].transpose(1,2,0)*a.std+a.mean)[...,::-1])\n",
    "    plt.show()\n",
    "    for pic in pics:\n",
    "        if pic.sum()>0:\n",
    "            plt.imshow(pic)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    b=input('s')\n",
    "    show(int(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "    plt.imshow((a[i]['input'].transpose(1,2,0)*a.std+a.mean)[...,::-1])\n",
    "    plt.show()\n",
    "    b=input('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0]['input'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in a[0].items():\n",
    "#     print(k)\n",
    "# print(a[0]['bboxs'])\n",
    "# print(a[0]['bboxs'])\n",
    "# print(a[0]['wh'])\n",
    "# print(a[0]['hm'])\n",
    "for pic in a[0]['hm']['hm_32']:\n",
    "    print('hm_32')\n",
    "    if pic.sum()>0:\n",
    "        plt.imshow(pic)\n",
    "        plt.show()\n",
    "for pic in a[0]['hm']['hm_64']:\n",
    "    print('hm_64')\n",
    "    if pic.sum()>0:\n",
    "        plt.imshow(pic)\n",
    "        plt.show()\n",
    "for pic in a[0]['hm']['hm_128']:\n",
    "    print('hm_128')\n",
    "    if pic.sum()>0:\n",
    "        plt.imshow(pic)\n",
    "        plt.show()\n",
    "for pic in a[0]['hm']['hm_256']:\n",
    "    print('hm_256')\n",
    "    if pic.sum()>0:\n",
    "        plt.imshow(pic)\n",
    "        plt.show()\n",
    "plt.imshow((a[0]['input'].transpose(1,2,0)*a.std+a.mean)[...,::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs_32=dict(pics_64=[],pics_128=[],pics_192=[],pics_256=[],pics_384=[],pics_512=[])\n",
    "objs_64=dict(pics_128=[],pics_192=[],pics_256=[],pics_384=[],pics_512=[])\n",
    "objs_128=dict(pics_256=[],pics_384=[],pics_512=[])\n",
    "objs_256=dict(pics_384=[],pics_512=[])\n",
    "count_40=0\n",
    "count_80=0\n",
    "count_160=0\n",
    "count_256=0\n",
    "for index,j in enumerate(a):\n",
    "    for obj_index,(w,h) in enumerate(j['wh']):\n",
    "        w,h=w*4,h*4\n",
    "        if w==0 and h==0:\n",
    "            break\n",
    "        if min(w,h)<=40:\n",
    "            count_40+=1\n",
    "            if max(w,h)<=64:\n",
    "                objs_32['pics_64'].append(dict(pic_index=index,obj_index=obj_index))\n",
    "            elif max(w,h)<=128:\n",
    "                objs_32['pics_128'].append(dict(pic_index=index,obj_index=obj_index))\n",
    "            elif max(w,h)<=192:\n",
    "                objs_32['pics_192'].append(dict(pic_index=index,obj_index=obj_index))\n",
    "            elif max(w,h)<=256:\n",
    "                objs_32['pics_256'].append(dict(pic_index=index,obj_index=obj_index))\n",
    "            elif max(w,h)<=384:\n",
    "                objs_32['pics_384'].append(dict(pic_index=index,obj_index=obj_index))\n",
    "            else:\n",
    "                objs_32['pics_512'].append(dict(pic_index=index,obj_index=obj_index))\n",
    "        elif min(w,h)<=80:\n",
    "            count_80+=1\n",
    "            if max(w,h)<=128:\n",
    "                objs_64['pics_128'].append(dict(pic_index=index,obj_index=obj_index))\n",
    "            elif max(w,h)<=192:\n",
    "                objs_64['pics_192'].append(dict(pic_index=index,obj_index=obj_index))\n",
    "            elif max(w,h)<=256:\n",
    "                objs_64['pics_256'].append(dict(pic_index=index,obj_index=obj_index))\n",
    "            elif max(w,h)<=384:\n",
    "                objs_64['pics_384'].append(dict(pic_index=index,obj_index=obj_index))\n",
    "            else:\n",
    "                objs_64['pics_512'].append(dict(pic_index=index,obj_index=obj_index))\n",
    "        elif min(w,h)<=160:\n",
    "            count_160+=1\n",
    "            if max(w,h)<=256:\n",
    "                objs_128['pics_256'].append(dict(pic_index=index,obj_index=obj_index))\n",
    "            elif max(w,h)<=384:\n",
    "                objs_128['pics_384'].append(dict(pic_index=index,obj_index=obj_index))\n",
    "            elif max(w,h)<=512:\n",
    "                objs_128['pics_512'].append(dict(pic_index=index,obj_index=obj_index))\n",
    "        else:\n",
    "            count_256+=1\n",
    "            if max(w,h)<=384:\n",
    "                objs_256['pics_384'].append(dict(pic_index=index,obj_index=obj_index))\n",
    "            elif max(w,h)<=512:\n",
    "                objs_256['pics_512'].append(dict(pic_index=index,obj_index=obj_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(objs,'./objs_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(num):\n",
    "    wh=a[objs_32['pics_64'][num]['pic_index']]['wh'][objs_32['pics_64'][num]['obj_index']]*4\n",
    "    return wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i=0 \n",
    "while True:\n",
    "    i+=1 \n",
    "    show(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(num):\n",
    "    pic,wh,bbox,obj_pic,crop_pic,bbox_crop,hm,bg=dataset[num]\n",
    "    print(obj_pic.shape,bbox_crop,bbox,wh)\n",
    "    plt.subplot(1,5,1)\n",
    "    plt.imshow((obj_pic.transpose(1,2,0)*a.std+a.mean)[...,::-1])\n",
    "    plt.subplot(1,5,2)\n",
    "    plt.imshow((pic.transpose(1,2,0)*a.std+a.mean)[...,::-1])\n",
    "    plt.subplot(1,5,3)\n",
    "    plt.imshow((crop_pic.transpose(1,2,0)*a.std+a.mean)[...,::-1])\n",
    "    plt.subplot(1,5,4)\n",
    "    plt.imshow(hm.squeeze(0))\n",
    "    plt.subplot(1,5,5)\n",
    "    plt.imshow(bg.squeeze(0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader(dataset, batch_size=args.batch_size_test, shuffle=False, num_workers=args.workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b=a[0]['meta']\n",
    "bboxs=b['gt_det']\n",
    "pics1=a[0]['hm'][0]\n",
    "for bbox in bboxs:\n",
    "    pics1[int(bbox[1]):int(bbox[3]),int(bbox[0]):int(bbox[2]),]=1\n",
    "plt.imshow(pics1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=3,\n",
    "                     stride=stride,\n",
    "                     padding=1,\n",
    "                     bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if stride != 1:\n",
    "            self.conv1 = nn.Conv2d(inplanes,\n",
    "                                   planes,\n",
    "                                   kernel_size=4,\n",
    "                                   stride=stride,\n",
    "                                   padding=1,\n",
    "                                   bias=False)\n",
    "        else:\n",
    "            self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes,\n",
    "                               planes,\n",
    "                               kernel_size=3,\n",
    "                               stride=stride,\n",
    "                               padding=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetMR(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=21):\n",
    "        super(ResNetMR, self).__init__()\n",
    "        self.inplanes = 32\n",
    "        self.conv1 = nn.Conv2d(3,\n",
    "                               32,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 32, layers[0], stride=1)\n",
    "        self.layer1_4=self._make_layer(block,128,layers[0], stride=1)\n",
    "        self.inplanes=32\n",
    "        self.layer2 = self._make_layer(block, 64, layers[1], stride=2)\n",
    "        self.layer2_4 = self._make_layer(block, 128, layers[1], stride=1)\n",
    "        self.inplanes = 64\n",
    "        self.layer3 = self._make_layer(block, 128, layers[2], stride=2)\n",
    "        self.layer3_4 = self._make_layer(block, 128, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 256, layers[3], stride=2)\n",
    "        self.layer5 = self._make_layer(block, 512, layers[4], stride=2)\n",
    "        self.layer6 = self._make_layer(block, 1024, layers[5], stride=2)\n",
    "        self.layer4_tr=nn.Sequential(nn.Conv2d(256,\n",
    "                                               512,\n",
    "                                               kernel_size=4,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               bias=False),\n",
    "                                     nn.BatchNorm2d(512),\n",
    "                                     nn.ReLU(inplace=True))\n",
    "        self.layer6_tr=nn.Sequential(nn.ConvTranspose2d(1024,\n",
    "                                                        512,\n",
    "                                                        kernel_size=4,\n",
    "                                                        stride=2,\n",
    "                                                        padding=1,\n",
    "                                                        bias=False),\n",
    "                                     nn.BatchNorm2d(512),\n",
    "                                     nn.ReLU(inplace=True))\n",
    "        self.inplanes=1536\n",
    "        self.layer_f=self._make_layer(block,768,2,stride=1)\n",
    "        self.final_cls=nn.Conv2d(768,num_classes,1,1)\n",
    "    def fusion(self,input_middle):\n",
    "        output_4=self.layer4(input_middle)\n",
    "        output_5=self.layer5(output_4)\n",
    "        output_6=self.layer6(output_5)\n",
    "        output_4=self.layer4_tr(output_4)\n",
    "        output_6=self.layer6_tr(output_6)\n",
    "        output=torch.cat([output_4,output_5,output_6],1)\n",
    "        output=self.layer_f(output)\n",
    "        output=self.final_cls(output)\n",
    "        return output\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            if stride != 1:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv2d(self.inplanes,\n",
    "                              planes * block.expansion,\n",
    "                              kernel_size=4,\n",
    "                              stride=stride,\n",
    "                              padding=1,\n",
    "                              bias=False),\n",
    "                    nn.BatchNorm2d(planes * block.expansion),\n",
    "                )\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv2d(self.inplanes,\n",
    "                              planes * block.expansion,\n",
    "                              kernel_size=1,\n",
    "                              stride=stride,\n",
    "                              bias=False),\n",
    "                    nn.BatchNorm2d(planes * block.expansion),\n",
    "                )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x,pattern,cal):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x1 = self.layer1(x)\n",
    "        self.x1=x1\n",
    "        if pattern==1:\n",
    "            x4=self.layer1_4(x1)\n",
    "            fm=self.fusion(x4)\n",
    "            return fm\n",
    "        \n",
    "        self.x2=self.layer2(self.x1)\n",
    "        if pattern==2:\n",
    "\n",
    "            x4=self.layer2_4(self.x2)\n",
    "            fm=self.fusion(x4)\n",
    "            return fm\n",
    "        \n",
    "        self.x3=self.layer3(self.x2)\n",
    "        if pattern==3:\n",
    "            fm=self.fusion(self.x3)\n",
    "            return fm\n",
    "        if pattern==4:\n",
    "            self.x4=self.layer3_4(self.x3)\n",
    "            fm=self.fusion(self.x4)\n",
    "            return fm\n",
    "\n",
    "def resnet_mr(pretrained=False,**kwargs):\n",
    "    model=ResNetMR(BasicBlock,[1,1,2,2,2,2,2],**kwargs)    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.1",
   "language": "python",
   "name": "pytorch1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
