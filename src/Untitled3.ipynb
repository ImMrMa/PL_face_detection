{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _init_paths\n",
    "from datasets.dataset_factory import get_dataset\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=get_dataset('wider_csp','fadet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_imgs: 12876\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "        data(mode='train'),\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        # collate_fn=default_collate\n",
    "    )\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).view(1,1,3)\n",
    "std  = torch.tensor([0.229, 0.224, 0.225]).view(1,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torchvision.models.utils import load_state_dict_from_url\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import os.path as osp\n",
    "import math\n",
    "__all__ = [\n",
    "    'ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152',\n",
    "    'resnext50_32x4d', 'resnext101_32x8d'\n",
    "]\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18':\n",
    "    'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34':\n",
    "    'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50':\n",
    "    'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101':\n",
    "    'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152':\n",
    "    'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "    'resnext50_32x4d':\n",
    "    'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
    "    'resnext101_32x8d':\n",
    "    'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def group_norm(in_planes):\n",
    "    return nn.GroupNorm(in_planes // 16, in_planes)\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=3,\n",
    "                     stride=stride,\n",
    "                     padding=dilation,\n",
    "                     groups=groups,\n",
    "                     bias=False,\n",
    "                     dilation=dilation)\n",
    "\n",
    "\n",
    "def conv4x4(in_planes, out_planes, stride=1, groups=1, dilation=1, padding=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=4,\n",
    "                     stride=stride,\n",
    "                     padding=padding,\n",
    "                     groups=groups,\n",
    "                     bias=False,\n",
    "                     dilation=dilation)\n",
    "\n",
    "\n",
    "def conv2x2(in_planes, out_planes, stride=1, groups=1, dilation=1, padding=0):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=2,\n",
    "                     stride=stride,\n",
    "                     padding=padding,\n",
    "                     groups=groups,\n",
    "                     bias=False,\n",
    "                     dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=1,\n",
    "                     stride=stride,\n",
    "                     bias=False)\n",
    "\n",
    "\n",
    "class L2Norm(nn.Module):\n",
    "    def __init__(self, inplanes, gamma_init=10):\n",
    "        super(L2Norm, self).__init__()\n",
    "        self.gamma_init = torch.Tensor(1, inplanes, 1, 1)\n",
    "        self.gamma_init[...] = gamma_init\n",
    "        self.gamma_init = Parameter(self.gamma_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        x = x * self.gamma_init\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self,\n",
    "                 inplanes,\n",
    "                 planes,\n",
    "                 stride=1,\n",
    "                 downsample=None,\n",
    "                 groups=1,\n",
    "                 base_width=64,\n",
    "                 dilation=1,\n",
    "                 norm_layer=None,\n",
    "                 replace_with_bn=False,\n",
    "                 layer_conv=False,layer4=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.norm_layer = norm_layer\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\n",
    "                'BasicBlock only supports groups=1 and base_width=64')\n",
    "        # if dilation > 1:\n",
    "        #     raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        if layer4:\n",
    "            layer_conv=dict(k=4,p=3,s=1,d=2)\n",
    "        if layer_conv :\n",
    "            self.conv1 = nn.Conv2d(inplanes,\n",
    "                                   planes,\n",
    "                                   kernel_size=layer_conv['k'],\n",
    "                                   stride=layer_conv['s'],\n",
    "                                   padding=layer_conv['p'],\n",
    "                                   groups=groups,\n",
    "                                   bias=False,\n",
    "                                   dilation=layer_conv['d'])\n",
    "        else:\n",
    "            self.conv1 = conv3x3(inplanes, planes, stride, dilation=dilation)\n",
    "        if replace_with_bn:\n",
    "            norm_layer = group_norm\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        norm_layer = self.norm_layer\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self,\n",
    "                 inplanes,\n",
    "                 planes,\n",
    "                 stride=1,\n",
    "                 downsample=None,\n",
    "                 groups=1,\n",
    "                 base_width=64,\n",
    "                 dilation=1,\n",
    "                 norm_layer=None,\n",
    "                 conv4=False,\n",
    "                 conv2=False,\n",
    "                 replace_with_bn=False):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.norm_layer = norm_layer\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        if conv4 and dilation == 1:\n",
    "            self.conv2 = conv4x4(width, width, stride, groups)\n",
    "        elif conv2 and dilation == 1:\n",
    "            self.conv2 = conv2x2(width, width, stride, groups)\n",
    "        elif dilation > 1:\n",
    "            self.conv1 = conv4x4(inplanes, planes, stride,\n",
    "                                 dilation=dilation, padding=3)\n",
    "        else:\n",
    "            self.conv1 = conv3x3(in_planes, out_planes)\n",
    "        if replace_with_bn:\n",
    "            norm_layer = group_norm\n",
    "        self.bn2 = norm_layer(width)\n",
    "        norm_layer = self.norm_layer\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 block,\n",
    "                 layers,\n",
    "                 layer_conf=dict(),\n",
    "                 num_classes=1000,\n",
    "                 zero_init_residual=False,\n",
    "                 groups=1,\n",
    "                 width_per_group=64,\n",
    "                 replace_stride_with_dilation=None,\n",
    "                 norm_layer=None,\n",
    "                 change_s1=False,\n",
    "                 replace_with_bn=False,\n",
    "                 all_gn=False, mask_layer=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.layer_conf = dict(\n",
    "            layer0=dict(conv=dict(k=4,p=1,d=1,s=2),down=dict(k=2, p=0, d=1, s=2)),\n",
    "            layer1=dict(conv=dict(k=4,p=1,d=1,s=2),\n",
    "            down=dict(k=2, p=0, d=1, s=2)),\n",
    "            layer2=dict(conv=dict(k=4, p=1, d=1, s=2),\n",
    "                        down=dict(k=2, p=0, d=1, s=2)),\n",
    "            layer3=dict(conv=dict(k=4, p=1, d=1, s=2),\n",
    "                        down=dict(k=2, p=0, d=1, s=2)),\n",
    "            layer4=dict(conv=dict(k=3, p=2, d=2, s=1),\n",
    "                        down=dict(k=1, p=0, d=1, s=1)))\n",
    "        print(self.layer_conf)\n",
    "        self.layer_conf.update(layer_conf)\n",
    "        layer_conf = self.layer_conf\n",
    "        if all_gn:\n",
    "            bn_layer = group_norm\n",
    "        else:\n",
    "            bn_layer = nn.BatchNorm2d\n",
    "        print(all_gn)\n",
    "        print(bn_layer)\n",
    "        input('s')\n",
    "\n",
    "        norm_layer = bn_layer\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        if block is BasicBlock:\n",
    "            transform_planes = 128\n",
    "        elif block is Bottleneck:\n",
    "            transform_planes = 256\n",
    "        else:\n",
    "            raise ValueError('not block')\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        self.change_s1 = change_s1\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        if change_s1:\n",
    "            if replace_with_bn:\n",
    "                norm_layer = group_norm\n",
    "            self.inplanes = 16\n",
    "            self.layeri0 = nn.Sequential(\n",
    "                nn.Conv2d(3,\n",
    "                          16,\n",
    "                          kernel_size=3,\n",
    "                          stride=1,\n",
    "                          padding=1,\n",
    "                          bias=False), norm_layer(self.inplanes),\n",
    "                nn.ReLU(inplace=True))\n",
    "            self.layeri1 = self._make_layer(block,\n",
    "                                            16,\n",
    "                                            1,\n",
    "                                            stride=1,\n",
    "                                            )\n",
    "            self.layer0 = self._make_layer(block,\n",
    "                                           32,\n",
    "                                           2,\n",
    "                                           stride=2,\n",
    "                                           layer_conf=layer_conf['layer0']\n",
    "                                           )\n",
    "            norm_layer = self._norm_layer\n",
    "            self.layer1 = self._make_layer(block,\n",
    "                                           64,\n",
    "                                           layers[0],\n",
    "                                           stride=2,\n",
    "                                           layer_conf=layer_conf['layer1']\n",
    "\n",
    "                                           )\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(3,\n",
    "                                   self.inplanes,\n",
    "                                   kernel_size=7,\n",
    "                                   stride=2,\n",
    "                                   padding=3,\n",
    "                                   bias=False)\n",
    "            self.bn1 = norm_layer(self.inplanes)\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "            self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "            self.layer1 = self._make_layer(block,\n",
    "                                           64,\n",
    "                                           layers[0],\n",
    "                                           stride=1,\n",
    "\n",
    "                                           )\n",
    "        self.inplanes_s2 = self.inplanes\n",
    "        self.layer2 = self._make_layer(block,\n",
    "                                       128,\n",
    "                                       layers[1],\n",
    "                                       stride=2,\n",
    "                                       layer_conf=layer_conf['layer2']\n",
    "                                       )\n",
    "        self.inplanes_s3 = self.inplanes\n",
    "        self.layer3 = self._make_layer(block,\n",
    "                                       256,\n",
    "                                       layers[2],\n",
    "                                       stride=2,\n",
    "                                       layer_conf=layer_conf['layer3']\n",
    "\n",
    "                                       )\n",
    "        self.inplanes_s4 = self.inplanes\n",
    "        self.layer4 = self._make_layer(block,\n",
    "                                       512,\n",
    "                                       layers[3],\n",
    "                                       stride=2,\n",
    "                                       layer_conf=layer_conf['layer4'],layer4=True)\n",
    "        self.inplanes_s5 = self.inplanes\n",
    "        self.s3_up = nn.ConvTranspose2d(in_channels=self.inplanes_s3,\n",
    "                                        out_channels=transform_planes,\n",
    "                                        kernel_size=4,\n",
    "                                        stride=2,\n",
    "                                        padding=1,\n",
    "                                        output_padding=0,\n",
    "                                        bias=False)\n",
    "        self.s4_up = nn.ConvTranspose2d(in_channels=self.inplanes_s4,\n",
    "                                        out_channels=transform_planes,\n",
    "                                        kernel_size=4,\n",
    "                                        stride=4,\n",
    "                                        padding=0,\n",
    "                                        output_padding=0,\n",
    "                                        bias=False)\n",
    "        self.s5_up = nn.ConvTranspose2d(in_channels=self.inplanes_s5,\n",
    "                                        out_channels=transform_planes,\n",
    "                                        kernel_size=4,\n",
    "                                        stride=4,\n",
    "                                        padding=0,\n",
    "                                        output_padding=0,\n",
    "                                        bias=False)\n",
    "        self.n3 = L2Norm(transform_planes)\n",
    "        self.n4 = L2Norm(transform_planes)\n",
    "        self.n5 = L2Norm(transform_planes)\n",
    "        self.s_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=transform_planes * 3,\n",
    "                      out_channels=transform_planes,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=False), norm_layer(transform_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.hm = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=transform_planes,\n",
    "                out_channels=1,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "            ), nn.Sigmoid())\n",
    "        self.wh = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=transform_planes,\n",
    "                out_channels=2,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "            ))\n",
    "        self.offset = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=transform_planes,\n",
    "                out_channels=2,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "            ))\n",
    "        self.mask_layer = mask_layer\n",
    "        if mask_layer:\n",
    "            self.mask = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=transform_planes,\n",
    "                    out_channels=1,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                ), nn.Sigmoid())\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.Conv2d):\n",
    "        #         nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        #     elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "        #         nn.init.constant_(m.weight, 1)\n",
    "        #         nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        # if zero_init_residual:\n",
    "        #     for m in self.modules():\n",
    "        #         if isinstance(m, Bottleneck):\n",
    "        #             nn.init.constant_(m.bn3.weight, 0)\n",
    "        #         elif isinstance(m, BasicBlock):\n",
    "        #             nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self,\n",
    "                    block,\n",
    "                    planes,\n",
    "                    blocks,\n",
    "                    stride=1,\n",
    "                    replace_with_bn=False,\n",
    "                    layer_conf=dict(conv=dict(k=3, p=1, s=1, d=1), down=dict(k=1, p=0, s=1, d=1)),layer4=False):\n",
    "        down = layer_conf['down']\n",
    "        conv = layer_conf['conv']\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        self.dilation = conv['d']\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            if replace_with_bn:\n",
    "                norm_layer = group_norm\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes,\n",
    "                          planes * block.expansion,\n",
    "                          kernel_size=down['k'],\n",
    "                          stride=down['s'],\n",
    "                          padding=down['p'],\n",
    "                          bias=False,\n",
    "                          dilation=down['d']),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "            norm_layer = self._norm_layer\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(self.inplanes,\n",
    "                  planes,\n",
    "                  stride,\n",
    "                  downsample,\n",
    "                  self.groups,\n",
    "                  self.base_width,\n",
    "                  norm_layer=norm_layer,\n",
    "                  layer_conv=conv,\n",
    "                  replace_with_bn=replace_with_bn))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(self.inplanes,\n",
    "                      planes,\n",
    "                      groups=self.groups,\n",
    "                      base_width=self.base_width,\n",
    "                      dilation=self.dilation,\n",
    "                      norm_layer=norm_layer,layer4=layer4))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.change_s1:\n",
    "            x = self.layeri0(x)\n",
    "            x = self.layeri1(x)\n",
    "            x = self.layer0(x)\n",
    "            s2 = self.layer1(x)\n",
    "        else:\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.maxpool(x)\n",
    "            s2 = self.layer1(x)\n",
    "        s3 = self.layer2(s2)\n",
    "        s4 = self.layer3(s3)\n",
    "        s5 = self.layer4(s4)\n",
    "        s3 = self.s3_up(s3)\n",
    "        s4 = self.s4_up(s4)\n",
    "        s5 = self.s5_up(s5)\n",
    "        s3 = self.n3(s3)\n",
    "        s4 = self.n4(s4)\n",
    "        s5 = self.n5(s5)\n",
    "        s_cat = torch.cat([s3, s4, s5], 1)\n",
    "        s_cat = self.s_conv(s_cat)\n",
    "        hm = self.hm(s_cat)\n",
    "        wh = self.wh(s_cat)\n",
    "        offset = self.offset(s_cat)\n",
    "        return_data = dict(hm=hm, wh=wh, offset=offset)\n",
    "        if self.mask_layer:\n",
    "            mask = self.mask(s_cat)\n",
    "            return_data['mask'] = mask\n",
    "        return return_data\n",
    "\n",
    "    def init_weights(\n",
    "            self,\n",
    "            pretrained='',\n",
    "    ):\n",
    "        if osp.isfile(pretrained):\n",
    "            pretrained_dict = torch.load(pretrained)\n",
    "            if 'state_dict' in pretrained_dict.keys():\n",
    "                pretrained_dict = pretrained_dict['state_dict']\n",
    "            elif 'model' in pretrained_dict.keys():\n",
    "                pretrained_dict = pretrained_dict['model']\n",
    "                new_dict = dict()\n",
    "                for k, v in pretrained_dict.items():\n",
    "                    new_dict[k[7:]] = v\n",
    "                pretrained_dict = new_dict\n",
    "        elif pretrained:\n",
    "            pretrained_dict = load_state_dict_from_url(\n",
    "                model_urls[pretrained], map_location='cpu')\n",
    "            # self.load_state_dict(pretrained_dict)\n",
    "        model_dict = self.state_dict()\n",
    "        if pretrained:\n",
    "            pretrained_dict = {\n",
    "                k: v\n",
    "                for k, v in pretrained_dict.items()\n",
    "                # and 'bn' not in k\n",
    "                if k in model_dict.keys() and v.shape == model_dict[k].shape\n",
    "            }\n",
    "            # for k,v in self.named_parameters():\n",
    "            #     v.requires_grad=False\n",
    "            #     if 'layer2' in k:\n",
    "            #         break\n",
    "        else:\n",
    "            pretrained_dict = self.state_dict()\n",
    "        # for k,v in self.named_parameters():\n",
    "        #     if 'bn' in k:\n",
    "        #         v.requires_grad=False\n",
    "        for k, v in self.named_parameters():\n",
    "            if 'hm' in k:\n",
    "                if 'bias' in k:\n",
    "                    pretrained_dict[k] = torch.ones_like(v) * -math.log(\n",
    "                        (1 - 0.01) / 0.01)\n",
    "            print(k, v.requires_grad)\n",
    "        input('grad:')\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)\n",
    "        for k, v in self.named_parameters():\n",
    "            print(k, v.shape)\n",
    "        input('model_parameters:')\n",
    "        for k, v in pretrained_dict.items():\n",
    "            print(k, v.shape)\n",
    "        input('pretrained_parameters')\n",
    "\n",
    "\n",
    "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    print(kwargs)\n",
    "    input('s')\n",
    "    if pretrained:\n",
    "        model.init_weights(pretrained=arch)\n",
    "    print(model)\n",
    "    input('s')\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(pretrained=True, progress=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet18',\n",
    "                   BasicBlock, [2, 2, 2, 2],\n",
    "                   pretrained,\n",
    "                   progress,\n",
    "                   replace_stride_with_dilation=[True, False, True],\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet50(pretrained=True, progress=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet50',\n",
    "                   Bottleneck, [3, 4, 6, 3],\n",
    "                   pretrained,\n",
    "                   progress,\n",
    "                   replace_stride_with_dilation=[True, False, True],\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def test():\n",
    "    import torch\n",
    "    net = resnet18(layer_conf=dict())\n",
    "    for k, v in net.named_parameters():\n",
    "        print(k)\n",
    "    print(net)\n",
    "    a = torch.ones(1, 3, 255, 255)\n",
    "    b = net(a)\n",
    "    for name, output in b.items():\n",
    "        print(name, output.shape)\n",
    "    input('s')\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torchvision.models.utils import load_state_dict_from_url\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import os.path as osp\n",
    "import math\n",
    "__all__ = [\n",
    "    'ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152',\n",
    "    'resnext50_32x4d', 'resnext101_32x8d'\n",
    "]\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18':\n",
    "    'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34':\n",
    "    'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50':\n",
    "    'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101':\n",
    "    'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152':\n",
    "    'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "    'resnext50_32x4d':\n",
    "    'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
    "    'resnext101_32x8d':\n",
    "    'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def group_norm(in_planes):\n",
    "    return nn.GroupNorm(in_planes // 16, in_planes)\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=3,\n",
    "                     stride=stride,\n",
    "                     padding=dilation,\n",
    "                     groups=groups,\n",
    "                     bias=False,\n",
    "                     dilation=dilation)\n",
    "\n",
    "\n",
    "def conv4x4(in_planes, out_planes, stride=1, groups=1, dilation=1, padding=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=4,\n",
    "                     stride=stride,\n",
    "                     padding=padding,\n",
    "                     groups=groups,\n",
    "                     bias=False,\n",
    "                     dilation=dilation)\n",
    "\n",
    "\n",
    "def conv2x2(in_planes, out_planes, stride=1, groups=1, dilation=1, padding=0):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=2,\n",
    "                     stride=stride,\n",
    "                     padding=padding,\n",
    "                     groups=groups,\n",
    "                     bias=False,\n",
    "                     dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=1,\n",
    "                     stride=stride,\n",
    "                     bias=False)\n",
    "\n",
    "\n",
    "class L2Norm(nn.Module):\n",
    "    def __init__(self, inplanes, gamma_init=10):\n",
    "        super(L2Norm, self).__init__()\n",
    "        self.gamma_init = torch.Tensor(1, inplanes, 1, 1)\n",
    "        self.gamma_init[...] = gamma_init\n",
    "        self.gamma_init = Parameter(self.gamma_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        x = x * self.gamma_init\n",
    "        return x\n",
    "\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, multi_large=False,):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        self.multi_large = multi_large\n",
    "\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(in_channels=16, out_channels=2, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "             nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3,\n",
    "                       stride=1, padding=1, bias=False),\n",
    "             nn.Conv2d(in_channels=16, out_channels=3, kernel_size=5,\n",
    "                       stride=1, padding=2, bias=False),\n",
    "             nn.Conv2d(in_channels=16, out_channels=3, kernel_size=7, stride=1, padding=3, bias=False)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = []\n",
    "        for conv_module in self.convs:\n",
    "            output.append(conv_module(x))\n",
    "        x = torch.cat(output, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self,\n",
    "                 inplanes,\n",
    "                 planes,\n",
    "                 stride=1,\n",
    "                 downsample=None,\n",
    "                 groups=1,\n",
    "                 base_width=64,\n",
    "                 dilation=1,\n",
    "                 norm_layer=None,\n",
    "                 replace_with_bn=False,\n",
    "                 layer_conv=False, layer4=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.norm_layer = norm_layer\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\n",
    "                'BasicBlock only supports groups=1 and base_width=64')\n",
    "        # if dilation > 1:\n",
    "        #     raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        if layer4:\n",
    "            layer_conv = dict(k=4, p=3, s=1, d=2)\n",
    "        if layer_conv:\n",
    "            self.conv1 = nn.Conv2d(inplanes,\n",
    "                                   planes,\n",
    "                                   kernel_size=layer_conv['k'],\n",
    "                                   stride=layer_conv['s'],\n",
    "                                   padding=layer_conv['p'],\n",
    "                                   groups=groups,\n",
    "                                   bias=False,\n",
    "                                   dilation=layer_conv['d'])\n",
    "        else:\n",
    "            self.conv1 = conv3x3(inplanes, planes, stride, dilation=dilation)\n",
    "        if replace_with_bn:\n",
    "            norm_layer = group_norm\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        norm_layer = self.norm_layer\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self,\n",
    "                 inplanes,\n",
    "                 planes,\n",
    "                 stride=1,\n",
    "                 downsample=None,\n",
    "                 groups=1,\n",
    "                 base_width=64,\n",
    "                 dilation=1,\n",
    "                 norm_layer=None,\n",
    "                 conv4=False,\n",
    "                 conv2=False,\n",
    "                 replace_with_bn=False):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.norm_layer = norm_layer\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        if conv4 and dilation == 1:\n",
    "            self.conv2 = conv4x4(width, width, stride, groups)\n",
    "        elif conv2 and dilation == 1:\n",
    "            self.conv2 = conv2x2(width, width, stride, groups)\n",
    "        elif dilation > 1:\n",
    "            self.conv1 = conv4x4(inplanes, planes, stride,\n",
    "                                 dilation=dilation, padding=3)\n",
    "        else:\n",
    "            self.conv1 = conv3x3(in_planes, out_planes)\n",
    "        if replace_with_bn:\n",
    "            norm_layer = group_norm\n",
    "        self.bn2 = norm_layer(width)\n",
    "        norm_layer = self.norm_layer\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 block,\n",
    "                 layers,\n",
    "                 layer_conf=dict(),\n",
    "                 num_classes=1000,\n",
    "                 zero_init_residual=False,\n",
    "                 groups=1,\n",
    "                 width_per_group=64,\n",
    "                 replace_stride_with_dilation=None,\n",
    "                 norm_layer=None,\n",
    "                 change_s1=False,\n",
    "                 replace_with_bn=False,\n",
    "                 all_gn=False, mulit_stage=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.mulit_stage = mulit_stage\n",
    "        self.layer_conf = dict(\n",
    "            layer0=dict(conv=dict(k=4, p=1, d=1, s=2),\n",
    "                        down=dict(k=2, p=0, d=1, s=2)),\n",
    "            layer1=dict(conv=dict(k=4, p=1, d=1, s=2),\n",
    "                        down=dict(k=2, p=0, d=1, s=2)),\n",
    "            layer2=dict(conv=dict(k=4, p=1, d=1, s=2),\n",
    "                        down=dict(k=2, p=0, d=1, s=2)),\n",
    "            layer3=dict(conv=dict(k=4, p=1, d=1, s=2),\n",
    "                        down=dict(k=2, p=0, d=1, s=2)),\n",
    "            layer4=dict(conv=dict(k=4, p=3, d=2, s=1),\n",
    "                        down=dict(k=1, p=0, d=1, s=1)))\n",
    "        layer_conf_conv4_conv2 = dict(conv=dict(k=4, p=1, d=1, s=2),\n",
    "                                      down=dict(k=2, p=0, d=1, s=2))\n",
    "        print(self.layer_conf)\n",
    "        self.layer_conf.update(layer_conf)\n",
    "        layer_conf = self.layer_conf\n",
    "        if all_gn:\n",
    "            bn_layer = group_norm\n",
    "        else:\n",
    "            bn_layer = nn.BatchNorm2d\n",
    "        print(all_gn)\n",
    "        print(bn_layer)\n",
    "        input('s')\n",
    "\n",
    "        norm_layer = bn_layer\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        if block is BasicBlock:\n",
    "            transform_planes = 128\n",
    "        elif block is Bottleneck:\n",
    "            transform_planes = 256\n",
    "        else:\n",
    "            raise ValueError('not block')\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        self.change_s1 = change_s1\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        if change_s1:\n",
    "            if replace_with_bn:\n",
    "                norm_layer = group_norm\n",
    "            self.inplanes = 16\n",
    "            self.layeri0 = nn.Sequential(\n",
    "                nn.Conv2d(3,\n",
    "                          16,\n",
    "                          kernel_size=3,\n",
    "                          stride=1,\n",
    "                          padding=1,\n",
    "                          bias=False), norm_layer(self.inplanes),\n",
    "                nn.ReLU(inplace=True))\n",
    "            self.layeri1 = self._make_layer(block,\n",
    "                                            16,\n",
    "                                            1,\n",
    "                                            stride=1,\n",
    "                                            )\n",
    "            self.layer0 = self._make_layer(block,\n",
    "                                           32,\n",
    "                                           2,\n",
    "                                           stride=2,\n",
    "                                           layer_conf=layer_conf['layer0']\n",
    "                                           )\n",
    "            norm_layer = self._norm_layer\n",
    "            self.layer1 = self._make_layer(block,\n",
    "                                           64,\n",
    "                                           layers[0],\n",
    "                                           stride=2,\n",
    "                                           layer_conf=layer_conf['layer1']\n",
    "                                           )\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(3,\n",
    "                                   self.inplanes,\n",
    "                                   kernel_size=7,\n",
    "                                   stride=2,\n",
    "                                   padding=3,\n",
    "                                   bias=False)\n",
    "            self.bn1 = norm_layer(self.inplanes)\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "            self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "            self.layer1 = self._make_layer(block,\n",
    "                                           64,\n",
    "                                           layers[0],\n",
    "                                           stride=1,\n",
    "                                           )\n",
    "        if mulit_stage:\n",
    "            self.inplanes = 16\n",
    "            self.layer_i0_2x = nn.Sequential(\n",
    "                nn.Conv2d(3,\n",
    "                          16,\n",
    "                          kernel_size=4,\n",
    "                          stride=2,\n",
    "                          padding=1,\n",
    "                          bias=False), norm_layer(self.inplanes),\n",
    "                nn.ReLU(inplace=True))\n",
    "            self.layer_i1_2x = nn.Sequential(\n",
    "                InceptionBlock(),\n",
    "                norm_layer(16),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            self.layer0_2x = self._make_layer(block,\n",
    "                                              32,\n",
    "                                              1,\n",
    "                                              stride=2,\n",
    "                                              layer_conf=layer_conf_conv4_conv2\n",
    "                                              )\n",
    "            self.layer1_2x = self._make_layer(block,\n",
    "                                              32,\n",
    "                                              2,\n",
    "                                              stride=2,\n",
    "                                              layer_conf=layer_conf_conv4_conv2\n",
    "                                              )\n",
    "            self.layer_fuse = nn.Sequential(\n",
    "                nn.Conv2d(96,\n",
    "                          64,\n",
    "                          kernel_size=3,\n",
    "                          stride=1,\n",
    "                          padding=1,\n",
    "                          bias=False), norm_layer(64),\n",
    "                nn.ReLU(inplace=True))\n",
    "        self.inplanes = 64\n",
    "        self.inplanes_s2 = self.inplanes\n",
    "        self.layer2 = self._make_layer(block,\n",
    "                                       128,\n",
    "                                       layers[1],\n",
    "                                       stride=2,\n",
    "                                       layer_conf=layer_conf['layer2']\n",
    "                                       )\n",
    "        self.inplanes_s3 = self.inplanes\n",
    "        self.layer3 = self._make_layer(block,\n",
    "                                       256,\n",
    "                                       layers[2],\n",
    "                                       stride=2,\n",
    "                                       layer_conf=layer_conf['layer3']\n",
    "                                       )\n",
    "        self.inplanes_s4 = self.inplanes\n",
    "        self.layer4 = self._make_layer(block,\n",
    "                                       512,\n",
    "                                       layers[3],\n",
    "                                       stride=2,\n",
    "                                       layer_conf=layer_conf['layer4'], layer4=True)\n",
    "        self.inplanes_s5 = self.inplanes\n",
    "        self.s3_up = nn.ConvTranspose2d(in_channels=self.inplanes_s3,\n",
    "                                        out_channels=transform_planes,\n",
    "                                        kernel_size=4,\n",
    "                                        stride=2,\n",
    "                                        padding=1,\n",
    "                                        output_padding=0,\n",
    "                                        bias=False)\n",
    "        self.s4_up = nn.ConvTranspose2d(in_channels=self.inplanes_s4,\n",
    "                                        out_channels=transform_planes,\n",
    "                                        kernel_size=4,\n",
    "                                        stride=4,\n",
    "                                        padding=0,\n",
    "                                        output_padding=0,\n",
    "                                        bias=False)\n",
    "        self.s5_up = nn.ConvTranspose2d(in_channels=self.inplanes_s5,\n",
    "                                        out_channels=transform_planes,\n",
    "                                        kernel_size=4,\n",
    "                                        stride=4,\n",
    "                                        padding=0,\n",
    "                                        output_padding=0,\n",
    "                                        bias=False)\n",
    "        self.n2 = L2Norm(64)\n",
    "        self.n3 = L2Norm(transform_planes)\n",
    "        self.n4 = L2Norm(transform_planes)\n",
    "        self.n5 = L2Norm(transform_planes)\n",
    "        self.n_cat = L2Norm(64)\n",
    "        self.cat_up = nn.ConvTranspose2d(in_channels=128,\n",
    "                                         out_channels=64,\n",
    "                                         kernel_size=4,\n",
    "                                         stride=2,\n",
    "                                         padding=1,\n",
    "                                         output_padding=0,\n",
    "                                         bias=False)\n",
    "        self.s2_up = nn.ConvTranspose2d(in_channels=64,\n",
    "                                        out_channels=64,\n",
    "                                        kernel_size=4,\n",
    "                                        stride=2,\n",
    "                                        padding=1,\n",
    "                                        output_padding=0,\n",
    "                                        bias=False)\n",
    "        self.s_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=transform_planes * 3,\n",
    "                      out_channels=transform_planes,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=False), norm_layer(transform_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.hm = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=transform_planes,\n",
    "                out_channels=1,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "            ), nn.Sigmoid())\n",
    "        self.wh = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=transform_planes,\n",
    "                out_channels=2,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "            ))\n",
    "        self.offset = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=transform_planes,\n",
    "                out_channels=2,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "            ))\n",
    "        self.s_conv_s = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=False), norm_layer(64),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.hm_s = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=1,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "            ), nn.Sigmoid())\n",
    "        self.wh_s = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=2,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "            ))\n",
    "        self.offset_s = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=2,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "            ))\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.Conv2d):\n",
    "        #         nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        #     elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "        #         nn.init.constant_(m.weight, 1)\n",
    "        #         nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        # if zero_init_residual:\n",
    "        #     for m in self.modules():\n",
    "        #         if isinstance(m, Bottleneck):\n",
    "        #             nn.init.constant_(m.bn3.weight, 0)\n",
    "        #         elif isinstance(m, BasicBlock):\n",
    "        #             nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self,\n",
    "                    block,\n",
    "                    planes,\n",
    "                    blocks,\n",
    "                    stride=1,\n",
    "                    replace_with_bn=False,\n",
    "                    layer_conf=dict(conv=dict(k=3, p=1, s=1, d=1), down=dict(k=1, p=0, s=1, d=1)), layer4=False):\n",
    "        down = layer_conf['down']\n",
    "        conv = layer_conf['conv']\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        self.dilation = conv['d']\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            if replace_with_bn:\n",
    "                norm_layer = group_norm\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes,\n",
    "                          planes * block.expansion,\n",
    "                          kernel_size=down['k'],\n",
    "                          stride=down['s'],\n",
    "                          padding=down['p'],\n",
    "                          bias=False,\n",
    "                          dilation=down['d']),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "            norm_layer = self._norm_layer\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(self.inplanes,\n",
    "                  planes,\n",
    "                  stride,\n",
    "                  downsample,\n",
    "                  self.groups,\n",
    "                  self.base_width,\n",
    "                  norm_layer=norm_layer,\n",
    "                  layer_conv=conv,\n",
    "                  replace_with_bn=replace_with_bn))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(self.inplanes,\n",
    "                      planes,\n",
    "                      groups=self.groups,\n",
    "                      base_width=self.base_width,\n",
    "                      dilation=self.dilation,\n",
    "                      norm_layer=norm_layer, layer4=layer4))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        img = x\n",
    "        if self.change_s1:\n",
    "            x = self.layeri0(x)\n",
    "            x = self.layeri1(x)\n",
    "            x = self.layer0(x)\n",
    "            s2 = self.layer1(x)\n",
    "        else:\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.maxpool(x)\n",
    "            s2 = self.layer1(x)\n",
    "        if self.mulit_stage:\n",
    "            x = nn.functional.interpolate(img, scale_factor=2, mode='bilinear')\n",
    "            x = self.layer_i0_2x(x)\n",
    "            x = self.layer_i1_2x(x)\n",
    "            x = self.layer0_2x(x)\n",
    "            s2_2x = self.layer1_2x(x)\n",
    "            s2 = torch.cat([s2, s2_2x], 1)\n",
    "            s2 = self.layer_fuse(s2)\n",
    "        s3 = self.layer2(s2)\n",
    "        s4 = self.layer3(s3)\n",
    "        s5 = self.layer4(s4)\n",
    "        s3 = self.s3_up(s3)\n",
    "        s4 = self.s4_up(s4)\n",
    "        s5 = self.s5_up(s5)\n",
    "        s3 = self.n3(s3)\n",
    "        s4 = self.n4(s4)\n",
    "        s5 = self.n5(s5)\n",
    "        s_cat = torch.cat([s3, s4, s5], 1)\n",
    "        s_cat = self.s_conv(s_cat)\n",
    "\n",
    "        hm = self.hm(s_cat)\n",
    "        wh = self.wh(s_cat)\n",
    "        offset = self.offset(s_cat)\n",
    "        s_cat = self.cat_up(s_cat)\n",
    "        s_cat = self.n_cat(s_cat)\n",
    "        s2 = self.s2_up(s2)\n",
    "        s2 = self.n2(s2)\n",
    "        s_cat_s = torch.cat([s_cat, s2], 1)\n",
    "        s_cat_s = self.s_conv_s(s_cat_s)\n",
    "        hm_small = self.hm_s(s_cat_s)\n",
    "        wh_small = self.wh_s(s_cat_s)\n",
    "        offset_small = self.offset_s(s_cat_s)\n",
    "        return_data = dict(hm=hm, wh=wh, offset=offset,hm_small=hm_small,wh_small=wh_small,offset_small=offset_small)\n",
    "        return return_data\n",
    "\n",
    "    def init_weights(\n",
    "            self,\n",
    "            pretrained='',\n",
    "    ):\n",
    "        if osp.isfile(pretrained):\n",
    "            pretrained_dict = torch.load(pretrained)\n",
    "            if 'state_dict' in pretrained_dict.keys():\n",
    "                pretrained_dict = pretrained_dict['state_dict']\n",
    "            elif 'model' in pretrained_dict.keys():\n",
    "                pretrained_dict = pretrained_dict['model']\n",
    "                new_dict = dict()\n",
    "                for k, v in pretrained_dict.items():\n",
    "                    new_dict[k[7:]] = v\n",
    "                pretrained_dict = new_dict\n",
    "        elif pretrained:\n",
    "            pretrained_dict = load_state_dict_from_url(\n",
    "                model_urls[pretrained], map_location='cpu')\n",
    "            # self.load_state_dict(pretrained_dict)\n",
    "        model_dict = self.state_dict()\n",
    "        if pretrained:\n",
    "            pretrained_dict = {\n",
    "                k: v\n",
    "                for k, v in pretrained_dict.items()\n",
    "                # and 'bn' not in k\n",
    "                if k in model_dict.keys() and v.shape == model_dict[k].shape\n",
    "            }\n",
    "            # for k,v in self.named_parameters():\n",
    "            #     v.requires_grad=False\n",
    "            #     if 'layer2' in k:\n",
    "            #         break\n",
    "        else:\n",
    "            pretrained_dict = self.state_dict()\n",
    "        for k,v in self.named_parameters():\n",
    "            if 'layer_i0_2x' in k:\n",
    "                break\n",
    "            v.requires_grad=False\n",
    "        for k, v in self.named_parameters():\n",
    "            # if 'bn' in k and k in pretrained_dict.keys():\n",
    "            #     v.requires_grad=False\n",
    "            print(k, v.shape)\n",
    "        input('model_parameters:')\n",
    "        for k, v in self.named_parameters():\n",
    "            if 'hm' in k:\n",
    "                if 'bias' in k:\n",
    "                    pretrained_dict[k] = torch.ones_like(v) * -math.log(\n",
    "                        (1 - 0.01) / 0.01)\n",
    "            print(k, v.requires_grad)\n",
    "        input('grad:')\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)\n",
    "\n",
    "\n",
    "        for k, v in pretrained_dict.items():\n",
    "            print(k, v.shape)\n",
    "        input('pretrained_parameters')\n",
    "\n",
    "\n",
    "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    print(kwargs)\n",
    "    input('s')\n",
    "    if pretrained:\n",
    "        model.init_weights(pretrained=arch)\n",
    "    print(model)\n",
    "    input('s')\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(pretrained=True, progress=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet18',\n",
    "                   BasicBlock, [2, 2, 2, 2],\n",
    "                   pretrained,\n",
    "                   progress,\n",
    "                   replace_stride_with_dilation=[True, False, True],\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet50(pretrained=True, progress=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet50',\n",
    "                   Bottleneck, [3, 4, 6, 3],\n",
    "                   pretrained,\n",
    "                   progress,\n",
    "                   replace_stride_with_dilation=[True, False, True],\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def test():\n",
    "    import torch\n",
    "    net = resnet18(mulit_stage=True, change_s1=True)\n",
    "    net.cuda()\n",
    "    for k, v in net.named_parameters():\n",
    "        print(k)\n",
    "    print(net)\n",
    "    a = torch.ones(5, 3, 704, 704).cuda()\n",
    "    while True:\n",
    "        b = net(a)\n",
    "        c = 0\n",
    "        for name, output in b.items():\n",
    "            c += output.sum()\n",
    "            print(name, output.shape)\n",
    "        c.backward()\n",
    "    input('s')\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(pretrained_path=None):\n",
    "    layer_conf=dict(layer2=dict(conv=dict(k=4,p=1,d=1,s=2),\n",
    "                    down=dict(k=2,p=0,d=1,s=2)),\n",
    "                   layer0=dict(conv=dict(k=4,p=1,d=1,s=2),\n",
    "                    down=dict(k=2,p=0,d=1,s=2)),\n",
    "                   layer4=dict(conv=dict(k=4,p=3,d=2,s=1),\n",
    "                    down=dict(k=1,p=0,d=1,s=1)))\n",
    "    model = net(mulit_stage=True, change_s1=True)\n",
    "    if pretrained_path:\n",
    "        print('loading weight!')\n",
    "        model_dict=torch.load(pretrained_path, map_location='cpu')['state_dict']\n",
    "        for k,v in model_dict.items():\n",
    "            print(k,v.shape)\n",
    "        input('s')\n",
    "        model.load_state_dict(model_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '6'\n",
    "cache_path = 'data/cache/widerface/val'\n",
    "out_path = 'data/eval/resnet18_s1_conv4_conv2_gn_last_easy'\n",
    "pretrained_path = '/data/users/mayx/my_code/github/CenterNet/models/model_180.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '6'\n",
    "cache_path = 'data/cache/widerface/val'\n",
    "out_path = 'data/eval/resnet18_s1_conv4_conv2_gn_last_easy'\n",
    "pretrained_path = '/data/users/mayx/my_code/github/CenterNet/models/model_180.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer0': {'conv': {'k': 4, 'p': 1, 'd': 1, 's': 2}, 'down': {'k': 2, 'p': 0, 'd': 1, 's': 2}}, 'layer1': {'conv': {'k': 4, 'p': 1, 'd': 1, 's': 2}, 'down': {'k': 2, 'p': 0, 'd': 1, 's': 2}}, 'layer2': {'conv': {'k': 4, 'p': 1, 'd': 1, 's': 2}, 'down': {'k': 2, 'p': 0, 'd': 1, 's': 2}}, 'layer3': {'conv': {'k': 4, 'p': 1, 'd': 1, 's': 2}, 'down': {'k': 2, 'p': 0, 'd': 1, 's': 2}}, 'layer4': {'conv': {'k': 4, 'p': 3, 'd': 2, 's': 1}, 'down': {'k': 1, 'p': 0, 'd': 1, 's': 1}}}\n",
      "False\n",
      "<class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "s\n",
      "{'replace_stride_with_dilation': [True, False, True], 'mulit_stage': True, 'change_s1': True}\n",
      "s\n",
      "layeri0.0.weight torch.Size([16, 3, 3, 3])\n",
      "layeri0.1.weight torch.Size([16])\n",
      "layeri0.1.bias torch.Size([16])\n",
      "layeri1.0.conv1.weight torch.Size([16, 16, 3, 3])\n",
      "layeri1.0.bn1.weight torch.Size([16])\n",
      "layeri1.0.bn1.bias torch.Size([16])\n",
      "layeri1.0.conv2.weight torch.Size([16, 16, 3, 3])\n",
      "layeri1.0.bn2.weight torch.Size([16])\n",
      "layeri1.0.bn2.bias torch.Size([16])\n",
      "layer0.0.conv1.weight torch.Size([32, 16, 4, 4])\n",
      "layer0.0.bn1.weight torch.Size([32])\n",
      "layer0.0.bn1.bias torch.Size([32])\n",
      "layer0.0.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "layer0.0.bn2.weight torch.Size([32])\n",
      "layer0.0.bn2.bias torch.Size([32])\n",
      "layer0.0.downsample.0.weight torch.Size([32, 16, 2, 2])\n",
      "layer0.0.downsample.1.weight torch.Size([32])\n",
      "layer0.0.downsample.1.bias torch.Size([32])\n",
      "layer0.1.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "layer0.1.bn1.weight torch.Size([32])\n",
      "layer0.1.bn1.bias torch.Size([32])\n",
      "layer0.1.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "layer0.1.bn2.weight torch.Size([32])\n",
      "layer0.1.bn2.bias torch.Size([32])\n",
      "layer1.0.conv1.weight torch.Size([64, 32, 4, 4])\n",
      "layer1.0.bn1.weight torch.Size([64])\n",
      "layer1.0.bn1.bias torch.Size([64])\n",
      "layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.weight torch.Size([64])\n",
      "layer1.0.bn2.bias torch.Size([64])\n",
      "layer1.0.downsample.0.weight torch.Size([64, 32, 2, 2])\n",
      "layer1.0.downsample.1.weight torch.Size([64])\n",
      "layer1.0.downsample.1.bias torch.Size([64])\n",
      "layer1.1.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn1.weight torch.Size([64])\n",
      "layer1.1.bn1.bias torch.Size([64])\n",
      "layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.weight torch.Size([64])\n",
      "layer1.1.bn2.bias torch.Size([64])\n",
      "layer_i0_2x.0.weight torch.Size([16, 3, 4, 4])\n",
      "layer_i0_2x.1.weight torch.Size([16])\n",
      "layer_i0_2x.1.bias torch.Size([16])\n",
      "layer_i1_2x.0.convs.0.weight torch.Size([2, 16, 1, 1])\n",
      "layer_i1_2x.0.convs.1.weight torch.Size([8, 16, 3, 3])\n",
      "layer_i1_2x.0.convs.2.weight torch.Size([3, 16, 5, 5])\n",
      "layer_i1_2x.0.convs.3.weight torch.Size([3, 16, 7, 7])\n",
      "layer_i1_2x.1.weight torch.Size([16])\n",
      "layer_i1_2x.1.bias torch.Size([16])\n",
      "layer0_2x.0.conv1.weight torch.Size([32, 16, 4, 4])\n",
      "layer0_2x.0.bn1.weight torch.Size([32])\n",
      "layer0_2x.0.bn1.bias torch.Size([32])\n",
      "layer0_2x.0.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "layer0_2x.0.bn2.weight torch.Size([32])\n",
      "layer0_2x.0.bn2.bias torch.Size([32])\n",
      "layer0_2x.0.downsample.0.weight torch.Size([32, 16, 2, 2])\n",
      "layer0_2x.0.downsample.1.weight torch.Size([32])\n",
      "layer0_2x.0.downsample.1.bias torch.Size([32])\n",
      "layer1_2x.0.conv1.weight torch.Size([32, 32, 4, 4])\n",
      "layer1_2x.0.bn1.weight torch.Size([32])\n",
      "layer1_2x.0.bn1.bias torch.Size([32])\n",
      "layer1_2x.0.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "layer1_2x.0.bn2.weight torch.Size([32])\n",
      "layer1_2x.0.bn2.bias torch.Size([32])\n",
      "layer1_2x.0.downsample.0.weight torch.Size([32, 32, 2, 2])\n",
      "layer1_2x.0.downsample.1.weight torch.Size([32])\n",
      "layer1_2x.0.downsample.1.bias torch.Size([32])\n",
      "layer1_2x.1.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "layer1_2x.1.bn1.weight torch.Size([32])\n",
      "layer1_2x.1.bn1.bias torch.Size([32])\n",
      "layer1_2x.1.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "layer1_2x.1.bn2.weight torch.Size([32])\n",
      "layer1_2x.1.bn2.bias torch.Size([32])\n",
      "layer_fuse.0.weight torch.Size([64, 96, 3, 3])\n",
      "layer_fuse.1.weight torch.Size([64])\n",
      "layer_fuse.1.bias torch.Size([64])\n",
      "layer2.0.conv1.weight torch.Size([128, 64, 4, 4])\n",
      "layer2.0.bn1.weight torch.Size([128])\n",
      "layer2.0.bn1.bias torch.Size([128])\n",
      "layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.0.bn2.weight torch.Size([128])\n",
      "layer2.0.bn2.bias torch.Size([128])\n",
      "layer2.0.downsample.0.weight torch.Size([128, 64, 2, 2])\n",
      "layer2.0.downsample.1.weight torch.Size([128])\n",
      "layer2.0.downsample.1.bias torch.Size([128])\n",
      "layer2.1.conv1.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn1.weight torch.Size([128])\n",
      "layer2.1.bn1.bias torch.Size([128])\n",
      "layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn2.weight torch.Size([128])\n",
      "layer2.1.bn2.bias torch.Size([128])\n",
      "layer3.0.conv1.weight torch.Size([256, 128, 4, 4])\n",
      "layer3.0.bn1.weight torch.Size([256])\n",
      "layer3.0.bn1.bias torch.Size([256])\n",
      "layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.0.bn2.weight torch.Size([256])\n",
      "layer3.0.bn2.bias torch.Size([256])\n",
      "layer3.0.downsample.0.weight torch.Size([256, 128, 2, 2])\n",
      "layer3.0.downsample.1.weight torch.Size([256])\n",
      "layer3.0.downsample.1.bias torch.Size([256])\n",
      "layer3.1.conv1.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn1.weight torch.Size([256])\n",
      "layer3.1.bn1.bias torch.Size([256])\n",
      "layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn2.weight torch.Size([256])\n",
      "layer3.1.bn2.bias torch.Size([256])\n",
      "layer4.0.conv1.weight torch.Size([512, 256, 4, 4])\n",
      "layer4.0.bn1.weight torch.Size([512])\n",
      "layer4.0.bn1.bias torch.Size([512])\n",
      "layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.0.bn2.weight torch.Size([512])\n",
      "layer4.0.bn2.bias torch.Size([512])\n",
      "layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "layer4.0.downsample.1.weight torch.Size([512])\n",
      "layer4.0.downsample.1.bias torch.Size([512])\n",
      "layer4.1.conv1.weight torch.Size([512, 512, 4, 4])\n",
      "layer4.1.bn1.weight torch.Size([512])\n",
      "layer4.1.bn1.bias torch.Size([512])\n",
      "layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn2.weight torch.Size([512])\n",
      "layer4.1.bn2.bias torch.Size([512])\n",
      "s3_up.weight torch.Size([128, 128, 4, 4])\n",
      "s4_up.weight torch.Size([256, 128, 4, 4])\n",
      "s5_up.weight torch.Size([512, 128, 4, 4])\n",
      "n2.gamma_init torch.Size([1, 64, 1, 1])\n",
      "n3.gamma_init torch.Size([1, 128, 1, 1])\n",
      "n4.gamma_init torch.Size([1, 128, 1, 1])\n",
      "n5.gamma_init torch.Size([1, 128, 1, 1])\n",
      "n_cat.gamma_init torch.Size([1, 64, 1, 1])\n",
      "cat_up.weight torch.Size([128, 64, 4, 4])\n",
      "s2_up.weight torch.Size([64, 64, 4, 4])\n",
      "s_conv.0.weight torch.Size([128, 384, 3, 3])\n",
      "s_conv.1.weight torch.Size([128])\n",
      "s_conv.1.bias torch.Size([128])\n",
      "hm.0.weight torch.Size([1, 128, 1, 1])\n",
      "hm.0.bias torch.Size([1])\n",
      "wh.0.weight torch.Size([2, 128, 1, 1])\n",
      "wh.0.bias torch.Size([2])\n",
      "offset.0.weight torch.Size([2, 128, 1, 1])\n",
      "offset.0.bias torch.Size([2])\n",
      "s_conv_s.0.weight torch.Size([64, 128, 3, 3])\n",
      "s_conv_s.1.weight torch.Size([64])\n",
      "s_conv_s.1.bias torch.Size([64])\n",
      "hm_s.0.weight torch.Size([1, 64, 1, 1])\n",
      "hm_s.0.bias torch.Size([1])\n",
      "wh_s.0.weight torch.Size([2, 64, 1, 1])\n",
      "wh_s.0.bias torch.Size([2])\n",
      "offset_s.0.weight torch.Size([2, 64, 1, 1])\n",
      "offset_s.0.bias torch.Size([2])\n",
      "model_parameters:\n",
      "layeri0.0.weight False\n",
      "layeri0.1.weight False\n",
      "layeri0.1.bias False\n",
      "layeri1.0.conv1.weight False\n",
      "layeri1.0.bn1.weight False\n",
      "layeri1.0.bn1.bias False\n",
      "layeri1.0.conv2.weight False\n",
      "layeri1.0.bn2.weight False\n",
      "layeri1.0.bn2.bias False\n",
      "layer0.0.conv1.weight False\n",
      "layer0.0.bn1.weight False\n",
      "layer0.0.bn1.bias False\n",
      "layer0.0.conv2.weight False\n",
      "layer0.0.bn2.weight False\n",
      "layer0.0.bn2.bias False\n",
      "layer0.0.downsample.0.weight False\n",
      "layer0.0.downsample.1.weight False\n",
      "layer0.0.downsample.1.bias False\n",
      "layer0.1.conv1.weight False\n",
      "layer0.1.bn1.weight False\n",
      "layer0.1.bn1.bias False\n",
      "layer0.1.conv2.weight False\n",
      "layer0.1.bn2.weight False\n",
      "layer0.1.bn2.bias False\n",
      "layer1.0.conv1.weight False\n",
      "layer1.0.bn1.weight False\n",
      "layer1.0.bn1.bias False\n",
      "layer1.0.conv2.weight False\n",
      "layer1.0.bn2.weight False\n",
      "layer1.0.bn2.bias False\n",
      "layer1.0.downsample.0.weight False\n",
      "layer1.0.downsample.1.weight False\n",
      "layer1.0.downsample.1.bias False\n",
      "layer1.1.conv1.weight False\n",
      "layer1.1.bn1.weight False\n",
      "layer1.1.bn1.bias False\n",
      "layer1.1.conv2.weight False\n",
      "layer1.1.bn2.weight False\n",
      "layer1.1.bn2.bias False\n",
      "layer_i0_2x.0.weight True\n",
      "layer_i0_2x.1.weight True\n",
      "layer_i0_2x.1.bias True\n",
      "layer_i1_2x.0.convs.0.weight True\n",
      "layer_i1_2x.0.convs.1.weight True\n",
      "layer_i1_2x.0.convs.2.weight True\n",
      "layer_i1_2x.0.convs.3.weight True\n",
      "layer_i1_2x.1.weight True\n",
      "layer_i1_2x.1.bias True\n",
      "layer0_2x.0.conv1.weight True\n",
      "layer0_2x.0.bn1.weight True\n",
      "layer0_2x.0.bn1.bias True\n",
      "layer0_2x.0.conv2.weight True\n",
      "layer0_2x.0.bn2.weight True\n",
      "layer0_2x.0.bn2.bias True\n",
      "layer0_2x.0.downsample.0.weight True\n",
      "layer0_2x.0.downsample.1.weight True\n",
      "layer0_2x.0.downsample.1.bias True\n",
      "layer1_2x.0.conv1.weight True\n",
      "layer1_2x.0.bn1.weight True\n",
      "layer1_2x.0.bn1.bias True\n",
      "layer1_2x.0.conv2.weight True\n",
      "layer1_2x.0.bn2.weight True\n",
      "layer1_2x.0.bn2.bias True\n",
      "layer1_2x.0.downsample.0.weight True\n",
      "layer1_2x.0.downsample.1.weight True\n",
      "layer1_2x.0.downsample.1.bias True\n",
      "layer1_2x.1.conv1.weight True\n",
      "layer1_2x.1.bn1.weight True\n",
      "layer1_2x.1.bn1.bias True\n",
      "layer1_2x.1.conv2.weight True\n",
      "layer1_2x.1.bn2.weight True\n",
      "layer1_2x.1.bn2.bias True\n",
      "layer_fuse.0.weight True\n",
      "layer_fuse.1.weight True\n",
      "layer_fuse.1.bias True\n",
      "layer2.0.conv1.weight True\n",
      "layer2.0.bn1.weight True\n",
      "layer2.0.bn1.bias True\n",
      "layer2.0.conv2.weight True\n",
      "layer2.0.bn2.weight True\n",
      "layer2.0.bn2.bias True\n",
      "layer2.0.downsample.0.weight True\n",
      "layer2.0.downsample.1.weight True\n",
      "layer2.0.downsample.1.bias True\n",
      "layer2.1.conv1.weight True\n",
      "layer2.1.bn1.weight True\n",
      "layer2.1.bn1.bias True\n",
      "layer2.1.conv2.weight True\n",
      "layer2.1.bn2.weight True\n",
      "layer2.1.bn2.bias True\n",
      "layer3.0.conv1.weight True\n",
      "layer3.0.bn1.weight True\n",
      "layer3.0.bn1.bias True\n",
      "layer3.0.conv2.weight True\n",
      "layer3.0.bn2.weight True\n",
      "layer3.0.bn2.bias True\n",
      "layer3.0.downsample.0.weight True\n",
      "layer3.0.downsample.1.weight True\n",
      "layer3.0.downsample.1.bias True\n",
      "layer3.1.conv1.weight True\n",
      "layer3.1.bn1.weight True\n",
      "layer3.1.bn1.bias True\n",
      "layer3.1.conv2.weight True\n",
      "layer3.1.bn2.weight True\n",
      "layer3.1.bn2.bias True\n",
      "layer4.0.conv1.weight True\n",
      "layer4.0.bn1.weight True\n",
      "layer4.0.bn1.bias True\n",
      "layer4.0.conv2.weight True\n",
      "layer4.0.bn2.weight True\n",
      "layer4.0.bn2.bias True\n",
      "layer4.0.downsample.0.weight True\n",
      "layer4.0.downsample.1.weight True\n",
      "layer4.0.downsample.1.bias True\n",
      "layer4.1.conv1.weight True\n",
      "layer4.1.bn1.weight True\n",
      "layer4.1.bn1.bias True\n",
      "layer4.1.conv2.weight True\n",
      "layer4.1.bn2.weight True\n",
      "layer4.1.bn2.bias True\n",
      "s3_up.weight True\n",
      "s4_up.weight True\n",
      "s5_up.weight True\n",
      "n2.gamma_init True\n",
      "n3.gamma_init True\n",
      "n4.gamma_init True\n",
      "n5.gamma_init True\n",
      "n_cat.gamma_init True\n",
      "cat_up.weight True\n",
      "s2_up.weight True\n",
      "s_conv.0.weight True\n",
      "s_conv.1.weight True\n",
      "s_conv.1.bias True\n",
      "hm.0.weight True\n",
      "hm.0.bias True\n",
      "wh.0.weight True\n",
      "wh.0.bias True\n",
      "offset.0.weight True\n",
      "offset.0.bias True\n",
      "s_conv_s.0.weight True\n",
      "s_conv_s.1.weight True\n",
      "s_conv_s.1.bias True\n",
      "hm_s.0.weight True\n",
      "hm_s.0.bias True\n",
      "wh_s.0.weight True\n",
      "wh_s.0.bias True\n",
      "offset_s.0.weight True\n",
      "offset_s.0.bias True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad:\n",
      "layer1.0.bn1.running_mean torch.Size([64])\n",
      "layer1.0.bn1.running_var torch.Size([64])\n",
      "layer1.0.bn1.weight torch.Size([64])\n",
      "layer1.0.bn1.bias torch.Size([64])\n",
      "layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.running_mean torch.Size([64])\n",
      "layer1.0.bn2.running_var torch.Size([64])\n",
      "layer1.0.bn2.weight torch.Size([64])\n",
      "layer1.0.bn2.bias torch.Size([64])\n",
      "layer1.1.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn1.running_mean torch.Size([64])\n",
      "layer1.1.bn1.running_var torch.Size([64])\n",
      "layer1.1.bn1.weight torch.Size([64])\n",
      "layer1.1.bn1.bias torch.Size([64])\n",
      "layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.running_mean torch.Size([64])\n",
      "layer1.1.bn2.running_var torch.Size([64])\n",
      "layer1.1.bn2.weight torch.Size([64])\n",
      "layer1.1.bn2.bias torch.Size([64])\n",
      "layer2.0.bn1.running_mean torch.Size([128])\n",
      "layer2.0.bn1.running_var torch.Size([128])\n",
      "layer2.0.bn1.weight torch.Size([128])\n",
      "layer2.0.bn1.bias torch.Size([128])\n",
      "layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.0.bn2.running_mean torch.Size([128])\n",
      "layer2.0.bn2.running_var torch.Size([128])\n",
      "layer2.0.bn2.weight torch.Size([128])\n",
      "layer2.0.bn2.bias torch.Size([128])\n",
      "layer2.0.downsample.1.running_mean torch.Size([128])\n",
      "layer2.0.downsample.1.running_var torch.Size([128])\n",
      "layer2.0.downsample.1.weight torch.Size([128])\n",
      "layer2.0.downsample.1.bias torch.Size([128])\n",
      "layer2.1.conv1.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn1.running_mean torch.Size([128])\n",
      "layer2.1.bn1.running_var torch.Size([128])\n",
      "layer2.1.bn1.weight torch.Size([128])\n",
      "layer2.1.bn1.bias torch.Size([128])\n",
      "layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn2.running_mean torch.Size([128])\n",
      "layer2.1.bn2.running_var torch.Size([128])\n",
      "layer2.1.bn2.weight torch.Size([128])\n",
      "layer2.1.bn2.bias torch.Size([128])\n",
      "layer3.0.bn1.running_mean torch.Size([256])\n",
      "layer3.0.bn1.running_var torch.Size([256])\n",
      "layer3.0.bn1.weight torch.Size([256])\n",
      "layer3.0.bn1.bias torch.Size([256])\n",
      "layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.0.bn2.running_mean torch.Size([256])\n",
      "layer3.0.bn2.running_var torch.Size([256])\n",
      "layer3.0.bn2.weight torch.Size([256])\n",
      "layer3.0.bn2.bias torch.Size([256])\n",
      "layer3.0.downsample.1.running_mean torch.Size([256])\n",
      "layer3.0.downsample.1.running_var torch.Size([256])\n",
      "layer3.0.downsample.1.weight torch.Size([256])\n",
      "layer3.0.downsample.1.bias torch.Size([256])\n",
      "layer3.1.conv1.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn1.running_mean torch.Size([256])\n",
      "layer3.1.bn1.running_var torch.Size([256])\n",
      "layer3.1.bn1.weight torch.Size([256])\n",
      "layer3.1.bn1.bias torch.Size([256])\n",
      "layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn2.running_mean torch.Size([256])\n",
      "layer3.1.bn2.running_var torch.Size([256])\n",
      "layer3.1.bn2.weight torch.Size([256])\n",
      "layer3.1.bn2.bias torch.Size([256])\n",
      "layer4.0.bn1.running_mean torch.Size([512])\n",
      "layer4.0.bn1.running_var torch.Size([512])\n",
      "layer4.0.bn1.weight torch.Size([512])\n",
      "layer4.0.bn1.bias torch.Size([512])\n",
      "layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.0.bn2.running_mean torch.Size([512])\n",
      "layer4.0.bn2.running_var torch.Size([512])\n",
      "layer4.0.bn2.weight torch.Size([512])\n",
      "layer4.0.bn2.bias torch.Size([512])\n",
      "layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "layer4.0.downsample.1.running_mean torch.Size([512])\n",
      "layer4.0.downsample.1.running_var torch.Size([512])\n",
      "layer4.0.downsample.1.weight torch.Size([512])\n",
      "layer4.0.downsample.1.bias torch.Size([512])\n",
      "layer4.1.bn1.running_mean torch.Size([512])\n",
      "layer4.1.bn1.running_var torch.Size([512])\n",
      "layer4.1.bn1.weight torch.Size([512])\n",
      "layer4.1.bn1.bias torch.Size([512])\n",
      "layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn2.running_mean torch.Size([512])\n",
      "layer4.1.bn2.running_var torch.Size([512])\n",
      "layer4.1.bn2.weight torch.Size([512])\n",
      "layer4.1.bn2.bias torch.Size([512])\n",
      "hm.0.bias torch.Size([1])\n",
      "hm_s.0.bias torch.Size([1])\n",
      "pretrained_parameters\n",
      "ResNet(\n",
      "  (layeri0): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "  )\n",
      "  (layeri1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer0): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer_i0_2x): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "  )\n",
      "  (layer_i1_2x): Sequential(\n",
      "    (0): InceptionBlock(\n",
      "      (convs): ModuleList(\n",
      "        (0): Conv2d(16, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (2): Conv2d(16, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (3): Conv2d(16, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "  )\n",
      "  (layer0_2x): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer1_2x): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer_fuse): Sequential(\n",
      "    (0): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), dilation=(2, 2), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), dilation=(2, 2), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (s3_up): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (s4_up): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
      "  (s5_up): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
      "  (n2): L2Norm()\n",
      "  (n3): L2Norm()\n",
      "  (n4): L2Norm()\n",
      "  (n5): L2Norm()\n",
      "  (n_cat): L2Norm()\n",
      "  (cat_up): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (s2_up): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (s_conv): Sequential(\n",
      "    (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "  )\n",
      "  (hm): Sequential(\n",
      "    (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (wh): Sequential(\n",
      "    (0): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (offset): Sequential(\n",
      "    (0): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (s_conv_s): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "  )\n",
      "  (hm_s): Sequential(\n",
      "    (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (wh_s): Sequential(\n",
      "    (0): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (offset_s): Sequential(\n",
      "    (0): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n",
      "loading weight!\n",
      "layeri0.0.weight torch.Size([16, 3, 3, 3])\n",
      "layeri0.1.weight torch.Size([16])\n",
      "layeri0.1.bias torch.Size([16])\n",
      "layeri0.1.running_mean torch.Size([16])\n",
      "layeri0.1.running_var torch.Size([16])\n",
      "layeri0.1.num_batches_tracked torch.Size([])\n",
      "layeri1.0.conv1.weight torch.Size([16, 16, 3, 3])\n",
      "layeri1.0.bn1.weight torch.Size([16])\n",
      "layeri1.0.bn1.bias torch.Size([16])\n",
      "layeri1.0.bn1.running_mean torch.Size([16])\n",
      "layeri1.0.bn1.running_var torch.Size([16])\n",
      "layeri1.0.bn1.num_batches_tracked torch.Size([])\n",
      "layeri1.0.conv2.weight torch.Size([16, 16, 3, 3])\n",
      "layeri1.0.bn2.weight torch.Size([16])\n",
      "layeri1.0.bn2.bias torch.Size([16])\n",
      "layeri1.0.bn2.running_mean torch.Size([16])\n",
      "layeri1.0.bn2.running_var torch.Size([16])\n",
      "layeri1.0.bn2.num_batches_tracked torch.Size([])\n",
      "layer0.0.conv1.weight torch.Size([32, 16, 4, 4])\n",
      "layer0.0.bn1.weight torch.Size([32])\n",
      "layer0.0.bn1.bias torch.Size([32])\n",
      "layer0.0.bn1.running_mean torch.Size([32])\n",
      "layer0.0.bn1.running_var torch.Size([32])\n",
      "layer0.0.bn1.num_batches_tracked torch.Size([])\n",
      "layer0.0.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "layer0.0.bn2.weight torch.Size([32])\n",
      "layer0.0.bn2.bias torch.Size([32])\n",
      "layer0.0.bn2.running_mean torch.Size([32])\n",
      "layer0.0.bn2.running_var torch.Size([32])\n",
      "layer0.0.bn2.num_batches_tracked torch.Size([])\n",
      "layer0.0.downsample.0.weight torch.Size([32, 16, 2, 2])\n",
      "layer0.0.downsample.1.weight torch.Size([32])\n",
      "layer0.0.downsample.1.bias torch.Size([32])\n",
      "layer0.0.downsample.1.running_mean torch.Size([32])\n",
      "layer0.0.downsample.1.running_var torch.Size([32])\n",
      "layer0.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "layer0.1.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "layer0.1.bn1.weight torch.Size([32])\n",
      "layer0.1.bn1.bias torch.Size([32])\n",
      "layer0.1.bn1.running_mean torch.Size([32])\n",
      "layer0.1.bn1.running_var torch.Size([32])\n",
      "layer0.1.bn1.num_batches_tracked torch.Size([])\n",
      "layer0.1.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "layer0.1.bn2.weight torch.Size([32])\n",
      "layer0.1.bn2.bias torch.Size([32])\n",
      "layer0.1.bn2.running_mean torch.Size([32])\n",
      "layer0.1.bn2.running_var torch.Size([32])\n",
      "layer0.1.bn2.num_batches_tracked torch.Size([])\n",
      "layer1.0.conv1.weight torch.Size([64, 32, 4, 4])\n",
      "layer1.0.bn1.weight torch.Size([64])\n",
      "layer1.0.bn1.bias torch.Size([64])\n",
      "layer1.0.bn1.running_mean torch.Size([64])\n",
      "layer1.0.bn1.running_var torch.Size([64])\n",
      "layer1.0.bn1.num_batches_tracked torch.Size([])\n",
      "layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.weight torch.Size([64])\n",
      "layer1.0.bn2.bias torch.Size([64])\n",
      "layer1.0.bn2.running_mean torch.Size([64])\n",
      "layer1.0.bn2.running_var torch.Size([64])\n",
      "layer1.0.bn2.num_batches_tracked torch.Size([])\n",
      "layer1.0.downsample.0.weight torch.Size([64, 32, 2, 2])\n",
      "layer1.0.downsample.1.weight torch.Size([64])\n",
      "layer1.0.downsample.1.bias torch.Size([64])\n",
      "layer1.0.downsample.1.running_mean torch.Size([64])\n",
      "layer1.0.downsample.1.running_var torch.Size([64])\n",
      "layer1.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "layer1.1.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn1.weight torch.Size([64])\n",
      "layer1.1.bn1.bias torch.Size([64])\n",
      "layer1.1.bn1.running_mean torch.Size([64])\n",
      "layer1.1.bn1.running_var torch.Size([64])\n",
      "layer1.1.bn1.num_batches_tracked torch.Size([])\n",
      "layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.weight torch.Size([64])\n",
      "layer1.1.bn2.bias torch.Size([64])\n",
      "layer1.1.bn2.running_mean torch.Size([64])\n",
      "layer1.1.bn2.running_var torch.Size([64])\n",
      "layer1.1.bn2.num_batches_tracked torch.Size([])\n",
      "layer_i0_2x.0.weight torch.Size([16, 3, 4, 4])\n",
      "layer_i0_2x.1.weight torch.Size([16])\n",
      "layer_i0_2x.1.bias torch.Size([16])\n",
      "layer_i0_2x.1.running_mean torch.Size([16])\n",
      "layer_i0_2x.1.running_var torch.Size([16])\n",
      "layer_i0_2x.1.num_batches_tracked torch.Size([])\n",
      "layer_i1_2x.0.convs.0.weight torch.Size([2, 16, 1, 1])\n",
      "layer_i1_2x.0.convs.1.weight torch.Size([8, 16, 3, 3])\n",
      "layer_i1_2x.0.convs.2.weight torch.Size([3, 16, 5, 5])\n",
      "layer_i1_2x.0.convs.3.weight torch.Size([3, 16, 7, 7])\n",
      "layer_i1_2x.1.weight torch.Size([16])\n",
      "layer_i1_2x.1.bias torch.Size([16])\n",
      "layer_i1_2x.1.running_mean torch.Size([16])\n",
      "layer_i1_2x.1.running_var torch.Size([16])\n",
      "layer_i1_2x.1.num_batches_tracked torch.Size([])\n",
      "layer0_2x.0.conv1.weight torch.Size([32, 16, 4, 4])\n",
      "layer0_2x.0.bn1.weight torch.Size([32])\n",
      "layer0_2x.0.bn1.bias torch.Size([32])\n",
      "layer0_2x.0.bn1.running_mean torch.Size([32])\n",
      "layer0_2x.0.bn1.running_var torch.Size([32])\n",
      "layer0_2x.0.bn1.num_batches_tracked torch.Size([])\n",
      "layer0_2x.0.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "layer0_2x.0.bn2.weight torch.Size([32])\n",
      "layer0_2x.0.bn2.bias torch.Size([32])\n",
      "layer0_2x.0.bn2.running_mean torch.Size([32])\n",
      "layer0_2x.0.bn2.running_var torch.Size([32])\n",
      "layer0_2x.0.bn2.num_batches_tracked torch.Size([])\n",
      "layer0_2x.0.downsample.0.weight torch.Size([32, 16, 2, 2])\n",
      "layer0_2x.0.downsample.1.weight torch.Size([32])\n",
      "layer0_2x.0.downsample.1.bias torch.Size([32])\n",
      "layer0_2x.0.downsample.1.running_mean torch.Size([32])\n",
      "layer0_2x.0.downsample.1.running_var torch.Size([32])\n",
      "layer0_2x.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "layer1_2x.0.conv1.weight torch.Size([32, 32, 4, 4])\n",
      "layer1_2x.0.bn1.weight torch.Size([32])\n",
      "layer1_2x.0.bn1.bias torch.Size([32])\n",
      "layer1_2x.0.bn1.running_mean torch.Size([32])\n",
      "layer1_2x.0.bn1.running_var torch.Size([32])\n",
      "layer1_2x.0.bn1.num_batches_tracked torch.Size([])\n",
      "layer1_2x.0.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "layer1_2x.0.bn2.weight torch.Size([32])\n",
      "layer1_2x.0.bn2.bias torch.Size([32])\n",
      "layer1_2x.0.bn2.running_mean torch.Size([32])\n",
      "layer1_2x.0.bn2.running_var torch.Size([32])\n",
      "layer1_2x.0.bn2.num_batches_tracked torch.Size([])\n",
      "layer1_2x.0.downsample.0.weight torch.Size([32, 32, 2, 2])\n",
      "layer1_2x.0.downsample.1.weight torch.Size([32])\n",
      "layer1_2x.0.downsample.1.bias torch.Size([32])\n",
      "layer1_2x.0.downsample.1.running_mean torch.Size([32])\n",
      "layer1_2x.0.downsample.1.running_var torch.Size([32])\n",
      "layer1_2x.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "layer1_2x.1.conv1.weight torch.Size([32, 32, 3, 3])\n",
      "layer1_2x.1.bn1.weight torch.Size([32])\n",
      "layer1_2x.1.bn1.bias torch.Size([32])\n",
      "layer1_2x.1.bn1.running_mean torch.Size([32])\n",
      "layer1_2x.1.bn1.running_var torch.Size([32])\n",
      "layer1_2x.1.bn1.num_batches_tracked torch.Size([])\n",
      "layer1_2x.1.conv2.weight torch.Size([32, 32, 3, 3])\n",
      "layer1_2x.1.bn2.weight torch.Size([32])\n",
      "layer1_2x.1.bn2.bias torch.Size([32])\n",
      "layer1_2x.1.bn2.running_mean torch.Size([32])\n",
      "layer1_2x.1.bn2.running_var torch.Size([32])\n",
      "layer1_2x.1.bn2.num_batches_tracked torch.Size([])\n",
      "layer_fuse.0.weight torch.Size([64, 96, 3, 3])\n",
      "layer_fuse.1.weight torch.Size([64])\n",
      "layer_fuse.1.bias torch.Size([64])\n",
      "layer_fuse.1.running_mean torch.Size([64])\n",
      "layer_fuse.1.running_var torch.Size([64])\n",
      "layer_fuse.1.num_batches_tracked torch.Size([])\n",
      "layer2.0.conv1.weight torch.Size([128, 64, 4, 4])\n",
      "layer2.0.bn1.weight torch.Size([128])\n",
      "layer2.0.bn1.bias torch.Size([128])\n",
      "layer2.0.bn1.running_mean torch.Size([128])\n",
      "layer2.0.bn1.running_var torch.Size([128])\n",
      "layer2.0.bn1.num_batches_tracked torch.Size([])\n",
      "layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.0.bn2.weight torch.Size([128])\n",
      "layer2.0.bn2.bias torch.Size([128])\n",
      "layer2.0.bn2.running_mean torch.Size([128])\n",
      "layer2.0.bn2.running_var torch.Size([128])\n",
      "layer2.0.bn2.num_batches_tracked torch.Size([])\n",
      "layer2.0.downsample.0.weight torch.Size([128, 64, 2, 2])\n",
      "layer2.0.downsample.1.weight torch.Size([128])\n",
      "layer2.0.downsample.1.bias torch.Size([128])\n",
      "layer2.0.downsample.1.running_mean torch.Size([128])\n",
      "layer2.0.downsample.1.running_var torch.Size([128])\n",
      "layer2.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "layer2.1.conv1.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn1.weight torch.Size([128])\n",
      "layer2.1.bn1.bias torch.Size([128])\n",
      "layer2.1.bn1.running_mean torch.Size([128])\n",
      "layer2.1.bn1.running_var torch.Size([128])\n",
      "layer2.1.bn1.num_batches_tracked torch.Size([])\n",
      "layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn2.weight torch.Size([128])\n",
      "layer2.1.bn2.bias torch.Size([128])\n",
      "layer2.1.bn2.running_mean torch.Size([128])\n",
      "layer2.1.bn2.running_var torch.Size([128])\n",
      "layer2.1.bn2.num_batches_tracked torch.Size([])\n",
      "layer3.0.conv1.weight torch.Size([256, 128, 4, 4])\n",
      "layer3.0.bn1.weight torch.Size([256])\n",
      "layer3.0.bn1.bias torch.Size([256])\n",
      "layer3.0.bn1.running_mean torch.Size([256])\n",
      "layer3.0.bn1.running_var torch.Size([256])\n",
      "layer3.0.bn1.num_batches_tracked torch.Size([])\n",
      "layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.0.bn2.weight torch.Size([256])\n",
      "layer3.0.bn2.bias torch.Size([256])\n",
      "layer3.0.bn2.running_mean torch.Size([256])\n",
      "layer3.0.bn2.running_var torch.Size([256])\n",
      "layer3.0.bn2.num_batches_tracked torch.Size([])\n",
      "layer3.0.downsample.0.weight torch.Size([256, 128, 2, 2])\n",
      "layer3.0.downsample.1.weight torch.Size([256])\n",
      "layer3.0.downsample.1.bias torch.Size([256])\n",
      "layer3.0.downsample.1.running_mean torch.Size([256])\n",
      "layer3.0.downsample.1.running_var torch.Size([256])\n",
      "layer3.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "layer3.1.conv1.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn1.weight torch.Size([256])\n",
      "layer3.1.bn1.bias torch.Size([256])\n",
      "layer3.1.bn1.running_mean torch.Size([256])\n",
      "layer3.1.bn1.running_var torch.Size([256])\n",
      "layer3.1.bn1.num_batches_tracked torch.Size([])\n",
      "layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn2.weight torch.Size([256])\n",
      "layer3.1.bn2.bias torch.Size([256])\n",
      "layer3.1.bn2.running_mean torch.Size([256])\n",
      "layer3.1.bn2.running_var torch.Size([256])\n",
      "layer3.1.bn2.num_batches_tracked torch.Size([])\n",
      "layer4.0.conv1.weight torch.Size([512, 256, 4, 4])\n",
      "layer4.0.bn1.weight torch.Size([512])\n",
      "layer4.0.bn1.bias torch.Size([512])\n",
      "layer4.0.bn1.running_mean torch.Size([512])\n",
      "layer4.0.bn1.running_var torch.Size([512])\n",
      "layer4.0.bn1.num_batches_tracked torch.Size([])\n",
      "layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.0.bn2.weight torch.Size([512])\n",
      "layer4.0.bn2.bias torch.Size([512])\n",
      "layer4.0.bn2.running_mean torch.Size([512])\n",
      "layer4.0.bn2.running_var torch.Size([512])\n",
      "layer4.0.bn2.num_batches_tracked torch.Size([])\n",
      "layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "layer4.0.downsample.1.weight torch.Size([512])\n",
      "layer4.0.downsample.1.bias torch.Size([512])\n",
      "layer4.0.downsample.1.running_mean torch.Size([512])\n",
      "layer4.0.downsample.1.running_var torch.Size([512])\n",
      "layer4.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "layer4.1.conv1.weight torch.Size([512, 512, 4, 4])\n",
      "layer4.1.bn1.weight torch.Size([512])\n",
      "layer4.1.bn1.bias torch.Size([512])\n",
      "layer4.1.bn1.running_mean torch.Size([512])\n",
      "layer4.1.bn1.running_var torch.Size([512])\n",
      "layer4.1.bn1.num_batches_tracked torch.Size([])\n",
      "layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn2.weight torch.Size([512])\n",
      "layer4.1.bn2.bias torch.Size([512])\n",
      "layer4.1.bn2.running_mean torch.Size([512])\n",
      "layer4.1.bn2.running_var torch.Size([512])\n",
      "layer4.1.bn2.num_batches_tracked torch.Size([])\n",
      "s3_up.weight torch.Size([128, 128, 4, 4])\n",
      "s4_up.weight torch.Size([256, 128, 4, 4])\n",
      "s5_up.weight torch.Size([512, 128, 4, 4])\n",
      "n2.gamma_init torch.Size([1, 64, 1, 1])\n",
      "n3.gamma_init torch.Size([1, 128, 1, 1])\n",
      "n4.gamma_init torch.Size([1, 128, 1, 1])\n",
      "n5.gamma_init torch.Size([1, 128, 1, 1])\n",
      "n_cat.gamma_init torch.Size([1, 64, 1, 1])\n",
      "cat_up.weight torch.Size([128, 64, 4, 4])\n",
      "s2_up.weight torch.Size([64, 64, 4, 4])\n",
      "s_conv.0.weight torch.Size([128, 384, 3, 3])\n",
      "s_conv.1.weight torch.Size([128])\n",
      "s_conv.1.bias torch.Size([128])\n",
      "s_conv.1.running_mean torch.Size([128])\n",
      "s_conv.1.running_var torch.Size([128])\n",
      "s_conv.1.num_batches_tracked torch.Size([])\n",
      "hm.0.weight torch.Size([1, 128, 1, 1])\n",
      "hm.0.bias torch.Size([1])\n",
      "wh.0.weight torch.Size([2, 128, 1, 1])\n",
      "wh.0.bias torch.Size([2])\n",
      "offset.0.weight torch.Size([2, 128, 1, 1])\n",
      "offset.0.bias torch.Size([2])\n",
      "s_conv_s.0.weight torch.Size([64, 128, 3, 3])\n",
      "s_conv_s.1.weight torch.Size([64])\n",
      "s_conv_s.1.bias torch.Size([64])\n",
      "s_conv_s.1.running_mean torch.Size([64])\n",
      "s_conv_s.1.running_var torch.Size([64])\n",
      "s_conv_s.1.num_batches_tracked torch.Size([])\n",
      "hm_s.0.weight torch.Size([1, 64, 1, 1])\n",
      "hm_s.0.bias torch.Size([1])\n",
      "wh_s.0.weight torch.Size([2, 64, 1, 1])\n",
      "wh_s.0.bias torch.Size([2])\n",
      "offset_s.0.weight torch.Size([2, 64, 1, 1])\n",
      "offset_s.0.bias torch.Size([2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n"
     ]
    }
   ],
   "source": [
    "net=resnet18\n",
    "model=get_model(pretrained_path=pretrained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_box(img, bbox):\n",
    "    length=((bbox[2]-bbox[0])*(bbox[3]-bbox[1]))**0.5\n",
    "    p=bbox[4]\n",
    "    w,h=(int(bbox[2]-bbox[0]),int(bbox[3]-bbox[1]))\n",
    "    center=(int((bbox[2]+bbox[0])/2),int((bbox[3]+bbox[1])/2))\n",
    "    bbox=(int(bbox[0]),int(bbox[1]),int(bbox[2]),int(bbox[3]))\n",
    "\n",
    "    color=(255-255*p,255-255*p,255*p)\n",
    "    if length<=12:\n",
    "        color=(255,255,255)\n",
    "    if length>12 and length<=16:\n",
    "        color=(0,0,0)\n",
    "#         cv2.ellipse(img, center, (w,h), 0, 0,360, (255, 255, 255), -1)\n",
    "    cv2.rectangle(img,(bbox[0], bbox[1]),(bbox[2], bbox[3]),color,1)\n",
    "    \n",
    "#     p=int(bbox[4]*10)\n",
    "#     img = cv2.putText(img, str(p),center, font, 1, (0, 255, 0), 2)\n",
    "    return img\n",
    "def vis_pred(img,boxes):\n",
    "    for box in boxes:\n",
    "        add_box(img,box)\n",
    "    return img\n",
    "        \n",
    "def parse_wider_offset(Y,img_h_new,img_w_new, score=0.1, down=4, nmsthre=0.5):\n",
    "\n",
    "    seman = Y[0][0, :, :, 0]\n",
    "    height = Y[1][0, :, :, 0]\n",
    "    width = Y[1][0, :, :, 1]\n",
    "    if down!=1:\n",
    "        offset_y = Y[2][0, :, :, 0]\n",
    "        offset_x = Y[2][0, :, :, 1]\n",
    "    y_c, x_c = np.where(seman > score)\n",
    "    boxs = []\n",
    "    if len(y_c) > 0:\n",
    "        for i in range(len(y_c)):\n",
    "            h = np.exp(height[y_c[i], x_c[i]]) * down\n",
    "            w = np.exp(width[y_c[i], x_c[i]]) * down\n",
    "            if down!=1:\n",
    "                o_y = offset_y[y_c[i], x_c[i]]\n",
    "                o_x = offset_x[y_c[i], x_c[i]]\n",
    "            else:\n",
    "                o_y=0\n",
    "                o_x=0\n",
    "            s = seman[y_c[i], x_c[i]]\n",
    "            x1, y1 = max(0, (x_c[i] + o_x + 0.5) * down - w / 2), max(\n",
    "                0, (y_c[i] + o_y + 0.5) * down - h / 2)\n",
    "            x1, y1 = min(x1, img_w_new), min(y1, img_h_new)\n",
    "            boxs.append([\n",
    "                x1, y1,\n",
    "                min(x1 + w, img_w_new),\n",
    "                min(y1 + h, img_h_new), s\n",
    "            ])\n",
    "        boxs = np.asarray(boxs, dtype=np.float32)\n",
    "        # keep = nms(boxs, nmsthre, usegpu=False, gpu_id=0)\n",
    "        # boxs = boxs[keep, :]\n",
    "        boxs = soft_bbox_vote(boxs, thre=nmsthre)\n",
    "        det=boxs\n",
    "        keep_index = np.where(\n",
    "        np.minimum(det[:, 2] - det[:, 0], det[:, 3] - det[:, 1]) >= 3)[0]\n",
    "        det = det[keep_index, :]\n",
    "        dets = soft_bbox_vote(det, thre=0.7)\n",
    "        keep_index = np.where((dets[:, 2] - dets[:, 0] + 1) *\n",
    "                              (dets[:, 3] - dets[:, 1] + 1) >= 6**2)[0]\n",
    "        dets = dets[keep_index, :]\n",
    "    return dets\n",
    "\n",
    "\n",
    "def soft_bbox_vote(det, thre=0.35, score=0.05):\n",
    "    if det.shape[0] <= 1:\n",
    "        return det\n",
    "    order = det[:, 4].ravel().argsort()[::-1]\n",
    "    det = det[order, :]\n",
    "    dets = []\n",
    "    while det.shape[0] > 0:\n",
    "        # IOU\n",
    "        area = (det[:, 2] - det[:, 0] + 1) * (det[:, 3] - det[:, 1] + 1)\n",
    "        xx1 = np.maximum(det[0, 0], det[:, 0])\n",
    "        yy1 = np.maximum(det[0, 1], det[:, 1])\n",
    "        xx2 = np.minimum(det[0, 2], det[:, 2])\n",
    "        yy2 = np.minimum(det[0, 3], det[:, 3])\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        o = inter / (area[0] + area[:] - inter)\n",
    "\n",
    "        # get needed merge det and delete these det\n",
    "        merge_index = np.where(o >= thre)[0]\n",
    "        det_accu = det[merge_index, :]\n",
    "        det_accu_iou = o[merge_index]\n",
    "        det = np.delete(det, merge_index, 0)\n",
    "\n",
    "        if merge_index.shape[0] <= 1:\n",
    "            try:\n",
    "                dets = np.row_stack((dets, det_accu))\n",
    "            except:\n",
    "                dets = det_accu\n",
    "            continue\n",
    "        else:\n",
    "            soft_det_accu = det_accu.copy()\n",
    "            soft_det_accu[:, 4] = soft_det_accu[:, 4] * (1 - det_accu_iou)\n",
    "            soft_index = np.where(soft_det_accu[:, 4] >= score)[0]\n",
    "            soft_det_accu = soft_det_accu[soft_index, :]\n",
    "\n",
    "            det_accu[:, 0:4] = det_accu[:, 0:4] * np.tile(\n",
    "                det_accu[:, -1:], (1, 4))\n",
    "            max_score = np.max(det_accu[:, 4])\n",
    "            det_accu_sum = np.zeros((1, 5))\n",
    "            det_accu_sum[:, 0:4] = np.sum(det_accu[:, 0:4], axis=0) / np.sum(\n",
    "                det_accu[:, -1:])\n",
    "            det_accu_sum[:, 4] = max_score\n",
    "\n",
    "            if soft_det_accu.shape[0] > 0:\n",
    "                det_accu_sum = np.row_stack((soft_det_accu, det_accu_sum))\n",
    "\n",
    "            try:\n",
    "                dets = np.row_stack((dets, det_accu_sum))\n",
    "            except:\n",
    "                dets = det_accu_sum\n",
    "\n",
    "    order = dets[:, 4].ravel().argsort()[::-1]\n",
    "    dets = dets[order, :]\n",
    "    return dets\n",
    "def detect_face_small_normal(img, scale=1, flip=False,nms=False):\n",
    "        img_h, img_w = img.shape[:2]\n",
    "        img_h_new, img_w_new = int(np.ceil(scale * img_h / 16) * 16), int(\n",
    "            np.ceil(scale * img_w / 16) * 16)\n",
    "        scale_h, scale_w = img_h_new / img_h, img_w_new / img_w\n",
    "\n",
    "        img_s = cv2.resize(img,\n",
    "                           None,\n",
    "                           None,\n",
    "                           fx=scale_w,\n",
    "                           fy=scale_h,\n",
    "                           interpolation=cv2.INTER_LINEAR)\n",
    "        # img_h, img_w = img_s.shape[:2]\n",
    "        # print frame_number\n",
    "\n",
    "\n",
    "        if flip:\n",
    "            img_ = cv2.flip(img_s, 1)\n",
    "            # x_rcnn = format_img_pad(img_sf, C)\n",
    "            img = pre_process(img_)\n",
    "        else:\n",
    "            # x_rcnn = format_img_pad(img_s, C)\n",
    "            img = pre_process(img_s)\n",
    "        with torch.no_grad():\n",
    "            output = model(img.cuda())\n",
    "            if nms:\n",
    "                output['hm']=_nms(output['hm'])\n",
    "            output=[output['hm'].cpu().detach().permute(0,2,3,1).numpy(),output['wh'].cpu().detach().permute(0,2,3,1).numpy(),output['offset'].cpu().detach().permute(0,2,3,1).numpy()]\n",
    "            if nms:\n",
    "                output['hm_small']=_nms(output['hm_small'])\n",
    "            output_samll=[output['hm_small'].cpu().detach().permute(0,2,3,1).numpy(),output['wh_small'].cpu().detach().permute(0,2,3,1).numpy(),output['offset_small'].cpu().detach().permute(0,2,3,1).numpy()]\n",
    "        boxes_small = parse_wider_offset(output_small,img_h_new,img_w_new,\n",
    "                                   score=0.05,\n",
    "                                   nmsthre=0.6)  \n",
    "        boxes = parse_wider_offset(output,img_h_new,img_w_new,\n",
    "                                   score=0.05,\n",
    "                                   nmsthre=0.6)\n",
    "        if len(boxes_small)>0:\n",
    "            keep_index = np.where(\n",
    "                np.minimum(boxes[:, 2] - boxes[:, 0], boxes[:, 3] -\n",
    "                           boxes[:, 1])<=20)[0]\n",
    "            boxes = boxes[keep_index, :]\n",
    "        if len(boxes) > 0:\n",
    "            keep_index = np.where(\n",
    "                np.minimum(boxes[:, 2] - boxes[:, 0], boxes[:, 3] -\n",
    "                           boxes[:, 1]) >= 16)[0]\n",
    "            boxes = boxes[keep_index, :]\n",
    "        boxes = np.row_stack((boxes_small, boxes))\n",
    "        if len(boxes) > 0:\n",
    "            if flip:\n",
    "                boxes[:, [0, 2]] = img_s.shape[1] - boxes[:, [2, 0]]\n",
    "            boxes[:, 0:4:2] = boxes[:, 0:4:2] / scale_w\n",
    "            boxes[:, 1:4:2] = boxes[:, 1:4:2] / scale_h\n",
    "        else:\n",
    "            boxes = np.empty(shape=[0, 5], dtype=np.float32)\n",
    "        return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 896)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish!!!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch.nn as nn\n",
    "def _nms(heat, kernel=3):\n",
    "    pad = (kernel - 1) // 2\n",
    "\n",
    "    hmax = nn.functional.max_pool2d(heat, (kernel, kernel),\n",
    "                                    stride=1,\n",
    "                                    padding=pad)\n",
    "    keep = (hmax == heat).float()\n",
    "    return heat * keep\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "img=cv2.imread('../worlds-largest-selfie.jpg')\n",
    "wh=(int(img.shape[1]*1//16)*16,int(img.shape[0]*1//16)*16)\n",
    "print(wh)\n",
    "img=cv2.resize(img,wh)\n",
    "img_ori=img.copy()\n",
    "img=img[:,:,[2,1,0]].astype(np.float32)\n",
    "\n",
    "h,w=img.shape[:2]\n",
    "img=img/255\n",
    "mean = np.array([0.485, 0.456, 0.406]).reshape(1, 1, 3).astype(np.float32)\n",
    "std = np.array([0.229, 0.224, 0.225]).reshape(1, 1, 3).astype(np.float32)\n",
    "\n",
    "img=(img-mean)/std\n",
    "img=img.transpose(2,0,1)\n",
    "img=torch.tensor(img)\n",
    "with torch.no_grad():\n",
    "    img=img.cuda()\n",
    "    img=img.unsqueeze(0)\n",
    "    output=model(img)\n",
    "def show_mask(output,img_ori):\n",
    "    mask=output['hm'][0].permute(1,2,0).cpu().detach().numpy()\n",
    "    mask=(255*(mask>0.05)*(mask<0.1)).astype(np.uint8)\n",
    "    mask=cv2.resize(mask,wh)\n",
    "#     mask=(255*((mask>0.02)*(mask<0.05))).astype(np.uint8)\n",
    "#     mask=((mask<0.2)*255).astype(np.uint8)+((mask>0.5)*255).astype(np.uint8)\n",
    "#     mask=cv2.resize(mask.copy(),wh)\n",
    "    cv2.imwrite('mask.jpg',mask)\n",
    "    input('s')\n",
    "    \n",
    "#     mask=mask.reshape(mask.shape[0],mask.shape[1],1)\n",
    "    print(mask.shape)\n",
    "    print(img_ori.shape)\n",
    "    img_ori[mask>1]=0\n",
    "    img=img_ori\n",
    "    print(img.shape)\n",
    "    plt.imshow(img)\n",
    "    cv2.imwrite('mask1.jpg',img)\n",
    "    print(mask.shape)\n",
    "    print(wh)\n",
    "    plt.show()\n",
    "    input('s')\n",
    "output['hm']=_nms(output['hm'])\n",
    "output['hm_small']=_nms(output['hm_small'])\n",
    "# show_mask(output,img_ori)\n",
    "hm=output['hm']\n",
    "# output['hm_small']=_nms(output['hm_small'])\n",
    "output_=[output['hm'].cpu().detach().permute(0,2,3,1).numpy(),output['wh'].cpu().detach().permute(0,2,3,1).numpy(),output['offset'].cpu().detach().permute(0,2,3,1).numpy()]\n",
    "output_small=[output['hm_small'].cpu().detach().permute(0,2,3,1).numpy(),output['wh_small'].cpu().detach().permute(0,2,3,1).numpy(),output['offset_small'].cpu().detach().permute(0,2,3,1).numpy()]\n",
    "boxes = parse_wider_offset(output_small,h,w,\n",
    "                                   score=0.05,\n",
    "                                   nmsthre=0.8,down=2)\n",
    "index = np.where(((boxes[:, 2] - boxes[:, 0] +\n",
    "               1)*(boxes[:, 3] - boxes[:, 1] +\n",
    "               1))**0.5<=20)[0]\n",
    "boxes=boxes[index,:]\n",
    "boxes2 = parse_wider_offset(output_,h,w,\n",
    "                                   score=0.05,\n",
    "                                   nmsthre=0.8,down=4)\n",
    "\n",
    "# hm_small=output['hm_small'][0][0].cpu().detach().numpy()\n",
    "# plt.imshow(hm_small)\n",
    "# plt.show()\n",
    "# input('s')\n",
    "# boxes=parse_wider_offset(output_small,h,w,\n",
    "#                                    score=0.05,down=1,nmsthre=0.8)\n",
    "index = np.where(((boxes2[:, 2] - boxes2[:, 0] +\n",
    "               1)*(boxes2[:, 3] - boxes2[:, 1] +\n",
    "               1))**0.5>=16)[0]\n",
    "boxes2 = boxes2[index, :]\n",
    "boxes = np.row_stack((boxes2, boxes))\n",
    "boxes = soft_bbox_vote(boxes, thre=0.4)\n",
    "hm=output['hm'].cpu().detach()[0][0].numpy()\n",
    "hm=(np.expand_dims(hm,-1).repeat(3,-1)*255).astype(np.uint8)\n",
    "img1=vis_pred(img_ori.copy(),boxes[:1000])\n",
    "cv2.imwrite('img1.jpg',img1)\n",
    "img2=vis_pred(img_ori.copy(),boxes2[:600])\n",
    "cv2.imwrite('img2.jpg',img2)\n",
    "# hm=(np.expand_dims(hm,-1).repeat(3,-1)*255).astype(np.uint8)\n",
    "plt.imshow(hm)\n",
    "cv2.imwrite('pic2.jpg',hm)\n",
    "# hm_small=(255-cv2.resize(hm_small,(wh))*255)\n",
    "# hm_small=hm_small.reshape(hm_small.shape[0],hm_small.shape[1],1)\n",
    "# img_ori=hm_small+img_ori\n",
    "\n",
    "cv2.imwrite('pic_hm2.jpg',hm)\n",
    "plt.show()\n",
    "print('finish!!!')\n",
    "# for data in val_loader:\n",
    "#     with torch.no_grad():\n",
    "#         out=model(data['input'].cuda())\n",
    "#         plt.imshow(data['input'][0].permute(1,2,0))\n",
    "#         plt.show()\n",
    "#         plt.imshow(data['hm'][0][0])\n",
    "#         cv2.imwrite('pic.jpg',data['hm'][0][0])\n",
    "#         plt.show()\n",
    "#         plt.imshow(out['hm'].cpu().detach()[0][0])\n",
    "#         plt.show()\n",
    "#         input('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class MaskLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskLoss, self).__init__()\n",
    "        self.bce_loss = F.binary_cross_entropy\n",
    "\n",
    "    def forward(self, output, label):\n",
    "        cls_loss = self.bce_loss(output[:, 0, ...],\n",
    "                                 label[:, 0, ...],\n",
    "                                 reduction='none')\n",
    "        positives = label[:, 0, :, :]\n",
    "        negatives = label[:, 0, :, :].eq(0).float()\n",
    "        foreground_weight = positives * ((1.0 - output[:, 0, :, :])**2)\n",
    "        background_weight = negatives * (\n",
    "            (1.0 - label[:, 0, :, :])**4.0) * (output[:, 0, :, :]**2)\n",
    "#         focal_weight = foreground_weight + background_weight\n",
    "        loss = torch.sum(foreground_weight * cls_loss)/torch.clamp(positives.sum(),1)+ torch.sum(background_weight * cls_loss)/torch.clamp(negatives.sum(),1)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_loss=MaskLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0321)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.zeros(1,1,704,704).float()\n",
    "b=torch.zeros(1,1,704,704).float()\n",
    "a[...]=0\n",
    "a[:,:,400:410,400:410]=0.7\n",
    "b[:,:,400:410,400:410]=1\n",
    "mask_loss(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os.path as osp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "def _nms(heat, kernel=3):\n",
    "    pad = (kernel - 1) // 2\n",
    "\n",
    "    hmax = nn.functional.max_pool2d(heat, (kernel, kernel),\n",
    "                                    stride=1,\n",
    "                                    padding=pad)\n",
    "    keep = (hmax == heat).float()\n",
    "    return heat * keep\n",
    "\n",
    "\n",
    "\n",
    "def parse_wider_offset(Y,img_h_new,img_w_new, score=0.1, down=4, nmsthre=0.5):\n",
    "    seman = Y[0][0, :, :, 0]\n",
    "    height = Y[1][0, :, :, 0]\n",
    "    width = Y[1][0, :, :, 1]\n",
    "    offset_y = Y[2][0, :, :, 0]\n",
    "    offset_x = Y[2][0, :, :, 1]\n",
    "    y_c, x_c = np.where(seman > score)\n",
    "    boxs = []\n",
    "    if len(y_c) > 0:\n",
    "        for i in range(len(y_c)):\n",
    "            h = np.exp(height[y_c[i], x_c[i]]) * down\n",
    "            w = np.exp(width[y_c[i], x_c[i]]) * down\n",
    "            o_y = offset_y[y_c[i], x_c[i]]\n",
    "            o_x = offset_x[y_c[i], x_c[i]]\n",
    "            s = seman[y_c[i], x_c[i]]\n",
    "            x1, y1 = max(0, (x_c[i] + o_x + 0.5) * down - w / 2), max(\n",
    "                0, (y_c[i] + o_y + 0.5) * down - h / 2)\n",
    "            x1, y1 = min(x1, img_w_new), min(y1, img_h_new)\n",
    "            boxs.append([\n",
    "                x1, y1,\n",
    "                min(x1 + w, img_w_new),\n",
    "                min(y1 + h, img_h_new), s\n",
    "            ])\n",
    "        boxs = np.asarray(boxs, dtype=np.float32)\n",
    "        # keep = nms(boxs, nmsthre, usegpu=False, gpu_id=0)\n",
    "        # boxs = boxs[keep, :]\n",
    "        boxs = soft_bbox_vote(boxs, thre=nmsthre)\n",
    "    return boxs\n",
    "\n",
    "\n",
    "def soft_bbox_vote(det, thre=0.35, score=0.05):\n",
    "    if det.shape[0] <= 1:\n",
    "        return det\n",
    "    order = det[:, 4].ravel().argsort()[::-1]\n",
    "    det = det[order, :]\n",
    "    dets = []\n",
    "    while det.shape[0] > 0:\n",
    "        # IOU\n",
    "        area = (det[:, 2] - det[:, 0] + 1) * (det[:, 3] - det[:, 1] + 1)\n",
    "        xx1 = np.maximum(det[0, 0], det[:, 0])\n",
    "        yy1 = np.maximum(det[0, 1], det[:, 1])\n",
    "        xx2 = np.minimum(det[0, 2], det[:, 2])\n",
    "        yy2 = np.minimum(det[0, 3], det[:, 3])\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        o = inter / (area[0] + area[:] - inter)\n",
    "\n",
    "        # get needed merge det and delete these det\n",
    "        merge_index = np.where(o >= thre)[0]\n",
    "        det_accu = det[merge_index, :]\n",
    "        det_accu_iou = o[merge_index]\n",
    "        det = np.delete(det, merge_index, 0)\n",
    "\n",
    "        if merge_index.shape[0] <= 1:\n",
    "            try:\n",
    "                dets = np.row_stack((dets, det_accu))\n",
    "            except:\n",
    "                dets = det_accu\n",
    "            continue\n",
    "        else:\n",
    "            soft_det_accu = det_accu.copy()\n",
    "            soft_det_accu[:, 4] = soft_det_accu[:, 4] * (1 - det_accu_iou)\n",
    "            soft_index = np.where(soft_det_accu[:, 4] >= score)[0]\n",
    "            soft_det_accu = soft_det_accu[soft_index, :]\n",
    "\n",
    "            det_accu[:, 0:4] = det_accu[:, 0:4] * np.tile(\n",
    "                det_accu[:, -1:], (1, 4))\n",
    "            max_score = np.max(det_accu[:, 4])\n",
    "            det_accu_sum = np.zeros((1, 5))\n",
    "            det_accu_sum[:, 0:4] = np.sum(det_accu[:, 0:4], axis=0) / np.sum(\n",
    "                det_accu[:, -1:])\n",
    "            det_accu_sum[:, 4] = max_score\n",
    "\n",
    "            if soft_det_accu.shape[0] > 0:\n",
    "                det_accu_sum = np.row_stack((soft_det_accu, det_accu_sum))\n",
    "\n",
    "            try:\n",
    "                dets = np.row_stack((dets, det_accu_sum))\n",
    "            except:\n",
    "                dets = det_accu_sum\n",
    "\n",
    "    order = dets[:, 4].ravel().argsort()[::-1]\n",
    "    dets = dets[order, :]\n",
    "    return dets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mean = np.array([0.485, 0.456, 0.406]).reshape(1, 1, 3).astype(np.float32)\n",
    "std = np.array([0.229, 0.224, 0.225]).reshape(1, 1, 3).astype(np.float32)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '7'\n",
    "cache_path = 'data/cache/widerface/val'\n",
    "out_path = 'data/eval/resnet18_s1_conv4_conv2_imagenet_nms'\n",
    "# pretrained_path = '/home/mayx/project/CenterNet/models/model_conv4_conv2_imagenet.pth'\n",
    "\n",
    "# model = get_model(pretrained_path)\n",
    "# model.cuda()\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "img = cv2.imread('../worlds-largest-selfie.jpg')\n",
    "\n",
    "def pre_process(img):\n",
    "\n",
    "    img = img.astype(np.float32)\n",
    "    img = img / 255\n",
    "    img = img[..., [2, 1, 0]]\n",
    "    img = (img - mean) / std\n",
    "    img = torch.tensor(img).permute(2,0,1).unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "def detect_face(img, scale=1, flip=False):\n",
    "    img_h, img_w = img.shape[:2]\n",
    "    img_h_new, img_w_new = int(np.ceil(scale * img_h / 16) * 16), int(\n",
    "        np.ceil(scale * img_w / 16) * 16)\n",
    "    scale_h, scale_w = img_h_new / img_h, img_w_new / img_w\n",
    "\n",
    "    img_s = cv2.resize(img,\n",
    "                       None,\n",
    "                       None,\n",
    "                       fx=scale_w,\n",
    "                       fy=scale_h,\n",
    "                       interpolation=cv2.INTER_LINEAR)\n",
    "    # img_h, img_w = img_s.shape[:2]\n",
    "    # print frame_number\n",
    "\n",
    "\n",
    "    if flip:\n",
    "        img_ = cv2.flip(img_s, 1)\n",
    "        # x_rcnn = format_img_pad(img_sf, C)\n",
    "        img = pre_process(img_)\n",
    "    else:\n",
    "        # x_rcnn = format_img_pad(img_s, C)\n",
    "        img = pre_process(img_s)\n",
    "    with torch.no_grad():\n",
    "        output = model(img.cuda())\n",
    "        output['hm']=_nms(output['hm'])\n",
    "        output=[output['hm'].cpu().detach().permute(0,2,3,1).numpy(),output['wh'].cpu().detach().permute(0,2,3,1).numpy(),output['offset'].cpu().detach().permute(0,2,3,1).numpy()]\n",
    "    boxes = parse_wider_offset(output,img_h_new,img_w_new,\n",
    "                               score=0.05,\n",
    "                               nmsthre=0.5)\n",
    "    if len(boxes) > 0:\n",
    "        keep_index = np.where(\n",
    "            np.minimum(boxes[:, 2] - boxes[:, 0], boxes[:, 3] -\n",
    "                       boxes[:, 1]) >= 12)[0]\n",
    "        boxes = boxes[keep_index, :]\n",
    "    if len(boxes) > 0:\n",
    "        if flip:\n",
    "            boxes[:, [0, 2]] = img_s.shape[1] - boxes[:, [2, 0]]\n",
    "        boxes[:, 0:4:2] = boxes[:, 0:4:2] / scale_w\n",
    "        boxes[:, 1:4:2] = boxes[:, 1:4:2] / scale_h\n",
    "    else:\n",
    "        boxes = np.empty(shape=[0, 5], dtype=np.float32)\n",
    "    return boxes\n",
    "def detect_face_small_normal(img, scale=1, flip=False,nms=False):\n",
    "    img_h, img_w = img.shape[:2]\n",
    "    img_h_new, img_w_new = int(np.ceil(scale * img_h / 16) * 16), int(\n",
    "        np.ceil(scale * img_w / 16) * 16)\n",
    "    scale_h, scale_w = img_h_new / img_h, img_w_new / img_w\n",
    "\n",
    "    img_s = cv2.resize(img,\n",
    "                       None,\n",
    "                       None,\n",
    "                       fx=scale_w,\n",
    "                       fy=scale_h,\n",
    "                       interpolation=cv2.INTER_LINEAR)\n",
    "    # img_h, img_w = img_s.shape[:2]\n",
    "    # print frame_number\n",
    "\n",
    "\n",
    "    if flip:\n",
    "        img_ = cv2.flip(img_s, 1)\n",
    "        # x_rcnn = format_img_pad(img_sf, C)\n",
    "        img = pre_process(img_)\n",
    "    else:\n",
    "        # x_rcnn = format_img_pad(img_s, C)\n",
    "        img = pre_process(img_s)\n",
    "    with torch.no_grad():\n",
    "        output = model(img.cuda())\n",
    "        if nms:\n",
    "            output['hm']=_nms(output['hm'])\n",
    "        if nms:\n",
    "            output['hm_small']=_nms(output['hm_small'])\n",
    "        output_s=[output['hm_small'].cpu().detach().permute(0,2,3,1).numpy(),output['wh_small'].cpu().detach().permute(0,2,3,1).numpy(),output['offset_small'].cpu().detach().permute(0,2,3,1).numpy()]\n",
    "        output_n=[output['hm'].cpu().detach().permute(0,2,3,1).numpy(),output['wh'].cpu().detach().permute(0,2,3,1).numpy(),output['offset'].cpu().detach().permute(0,2,3,1).numpy()]\n",
    "\n",
    "    boxes_s = parse_wider_offset(output_s,img_h_new,img_w_new,\n",
    "                               score=0.05,\n",
    "                               nmsthre=0.5,down=2)  \n",
    "    boxes_n = parse_wider_offset(output_n,img_h_new,img_w_new,\n",
    "                               score=0.05,\n",
    "                               nmsthre=0.5)\n",
    "    if len(boxes_s)>0:\n",
    "        keep_index = np.where(\n",
    "            np.minimum(boxes_s[:, 2] - boxes_s[:, 0], boxes_s[:, 3] -\n",
    "                       boxes_s[:, 1])<=20)[0]\n",
    "        boxes_s = boxes_s[keep_index, :]\n",
    "    if len(boxes_n) > 0:\n",
    "        keep_index = np.where(\n",
    "            np.minimum(boxes_n[:, 2] - boxes_n[:, 0], boxes_n[:, 3] -\n",
    "                       boxes_n[:, 1]) >= 16)[0]\n",
    "        boxes_n = boxes_n[keep_index, :]\n",
    "    boxes = np.row_stack((boxes_s, boxes_n))\n",
    "    if len(boxes) > 0:\n",
    "        if flip:\n",
    "            boxes[:, [0, 2]] = img_s.shape[1] - boxes[:, [2, 0]]\n",
    "        boxes[:, 0:4:2] = boxes[:, 0:4:2] / scale_w\n",
    "        boxes[:, 1:4:2] = boxes[:, 1:4:2] / scale_h\n",
    "    else:\n",
    "        boxes = np.empty(shape=[0, 5], dtype=np.float32)\n",
    "    return boxes\n",
    "def im_det_ms_pyramid(image, max_im_shrink):\n",
    "    # shrink detecting and shrink only detect big face\n",
    "    det_s = np.row_stack(\n",
    "        (detect_face(image, 0.5), detect_face(image, 0.5, flip=True)))\n",
    "    index = np.where(\n",
    "        np.maximum(det_s[:, 2] - det_s[:, 0] + 1, det_s[:, 3] -\n",
    "                   det_s[:, 1] + 1) > 64)[0]\n",
    "    det_s = det_s[index, :]\n",
    "\n",
    "    det_temp = np.row_stack(\n",
    "        (detect_face(image, 0.75), detect_face(image, 0.75, flip=True)))\n",
    "    index = np.where(\n",
    "        np.maximum(det_temp[:, 2] - det_temp[:, 0] + 1, det_temp[:, 3] -\n",
    "                   det_temp[:, 1] + 1) > 32)[0]\n",
    "    det_temp = det_temp[index, :]\n",
    "    det_s = np.row_stack((det_s, det_temp))\n",
    "\n",
    "    det_temp = np.row_stack(\n",
    "        (detect_face(image, 0.25), detect_face(image, 0.25, flip=True)))\n",
    "    index = np.where(\n",
    "        np.maximum(det_temp[:, 2] - det_temp[:, 0] + 1, det_temp[:, 3] -\n",
    "                   det_temp[:, 1] + 1) > 96)[0]\n",
    "    det_temp = det_temp[index, :]\n",
    "    det_s = np.row_stack((det_s, det_temp))\n",
    "\n",
    "    st = [1.25, 1.5, 1.75, 2.0, 2.25]\n",
    "    # st =[0.75,0.5,0.25] \n",
    "    for i in range(len(st)):\n",
    "        if (st[i] <= max_im_shrink):\n",
    "            det_temp = np.row_stack(\n",
    "                (detect_face(image,\n",
    "                             st[i]), detect_face(image, st[i], flip=True)))\n",
    "            # Enlarged images are only used to detect small faces.\n",
    "            if st[i] == 1.25:\n",
    "                index = np.where(\n",
    "                    np.minimum(det_temp[:, 2] - det_temp[:, 0] +\n",
    "                               1, det_temp[:, 3] - det_temp[:, 1] +\n",
    "                               1) < 128)[0]\n",
    "                det_temp = det_temp[index, :]\n",
    "            elif st[i] == 1.5:\n",
    "                index = np.where(\n",
    "                    np.minimum(det_temp[:, 2] - det_temp[:, 0] +\n",
    "                               1, det_temp[:, 3] - det_temp[:, 1] +\n",
    "                               1) < 96)[0]\n",
    "                det_temp = det_temp[index, :]\n",
    "            elif st[i] == 1.75:\n",
    "                index = np.where(\n",
    "                    np.minimum(det_temp[:, 2] - det_temp[:, 0] +\n",
    "                               1, det_temp[:, 3] - det_temp[:, 1] +\n",
    "                               1) < 64)[0]\n",
    "                det_temp = det_temp[index, :]\n",
    "            elif st[i] == 2.0:\n",
    "                index = np.where(\n",
    "                    np.minimum(det_temp[:, 2] - det_temp[:, 0] +\n",
    "                               1, det_temp[:, 3] - det_temp[:, 1] +\n",
    "                               1) < 48)[0]\n",
    "                det_temp = det_temp[index, :]\n",
    "            elif st[i] == 2.25:\n",
    "                index = np.where(\n",
    "                    np.minimum(det_temp[:, 2] - det_temp[:, 0] +\n",
    "                               1, det_temp[:, 3] - det_temp[:, 1] +\n",
    "                               1) < 32)[0]\n",
    "                det_temp = det_temp[index, :]\n",
    "            elif st[i] == 0.25:\n",
    "                index = np.where(\n",
    "                    np.minimum(det_temp[:, 2] - det_temp[:, 0] +\n",
    "                               1, det_temp[:, 3] - det_temp[:, 1] +\n",
    "                               1) < 64)[0]\n",
    "                det_temp = det_temp[index, :]\n",
    "            elif st[i] == 0.5:\n",
    "                index = np.where(\n",
    "                    np.minimum(det_temp[:, 2] - det_temp[:, 0] +\n",
    "                               1, det_temp[:, 3] - det_temp[:, 1] +\n",
    "                               1) < 128)[0]\n",
    "                det_temp = det_temp[index, :]\n",
    "            det_s = np.row_stack((det_s, det_temp))\n",
    "    return det_s\n",
    "\n",
    "max_im_shrink = (\n",
    "    0x7fffffff / 577.0 /\n",
    "    (img.shape[0] * img.shape[1]))**0.5  # the max size of input image\n",
    "shrink = max_im_shrink if max_im_shrink < 1 else 1\n",
    "det0 = detect_face_small_normal(img,nms=True)\n",
    "#     det1 = detect_face(img, flip=True)\n",
    "#     det2 = im_det_ms_pyramid(img, max_im_shrink)\n",
    "# merge all test results via bounding box voting\n",
    "#     det = np.row_stack((det0, det1, det2))\n",
    "det=det0\n",
    "keep_index = np.where(\n",
    "    ((det[:, 2] - det[:, 0])*(det[:, 3] - det[:, 1]))**0.5 >= 2)[0]\n",
    "# det = det[keep_index, :]\n",
    "dets = soft_bbox_vote(det, thre=0.4)\n",
    "# keep_index = np.where(\n",
    "#     np.minimum(dets[:, 2] - dets[:, 0], dets[:, 3] - dets[:, 1]) >= 3)[0]\n",
    "#     keep_index = np.where((dets[:, 2] - dets[:, 0] + 1) *\n",
    "#                           (dets[:, 3] - dets[:, 1] + 1) >= 6**2)[0]\n",
    "# dets = dets[keep_index, :]\n",
    "img=cv2.imread('../worlds-largest-selfie.jpg')\n",
    "img=vis_pred(img,dets[:1100])\n",
    "cv2.imwrite('img.jpg',img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
